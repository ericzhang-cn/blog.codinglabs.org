<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>CodingLabs</title>
        <link>http://blog.codinglabs.org</link>
        <description>keep coding, keep foolish</description>
        <lastBuildDate>Wed, 05 Aug 2015 14:08:14 +0800</lastBuildDate>
        <language>zh-cn</language>
        
        <item> 
            <title>如何实现一个malloc</title> 
            <link>http://blog.codinglabs.org/articles/a-malloc-tutorial.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/a-malloc-tutorial.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Tue, 19 Aug 2014 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;任何一个用过或学过C的人对malloc都不会陌生。大家都知道malloc可以分配一段连续的内存空间，并且在不再使用时可以通过free释放掉。但是，许多程序员对malloc背后的事情并不熟悉，许多人甚至把malloc当做操作系统所提供的系统调用或C的关键字。实际上，malloc只是C的标准库中提供的一个普通函数，而且实现malloc的&lt;strong&gt;基本&lt;/strong&gt;思想并不复杂，任何一个对C和操作系统有些许了解的程序员都可以很容易理解。&lt;/p&gt;
&lt;p&gt;这篇文章通过实现一个简单的malloc来描述malloc背后的机制。当然与现有C的标准库实现（例如glibc）相比，我们实现的malloc并不是特别高效，但是这个实现比目前真实的malloc实现要简单很多，因此易于理解。重要的是，这个实现和真实实现在基本原理上是一致的。&lt;/p&gt;
&lt;p&gt;这篇文章将首先介绍一些所需的基本知识，如操作系统对进程的内存管理以及相关的系统调用，然后逐步实现一个简单的malloc。为了简单起见，这篇文章将只考虑x86_64体系结构，操作系统为Linux。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;h1 id=&quot;1-什么是malloc&quot;&gt;1 什么是malloc&lt;/h1&gt;
&lt;p&gt;在实现malloc之前，先要相对正式地对malloc做一个定义。&lt;/p&gt;
&lt;p&gt;根据标准C库函数的定义，malloc具有如下原型：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;void* malloc(size_t size);
&lt;/pre&gt;
&lt;p&gt;这个函数要实现的功能是在系统中分配一段连续的可用的内存，具体有如下要求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;malloc分配的内存大小&lt;strong&gt;至少&lt;/strong&gt;为size参数所指定的字节数&lt;/li&gt;
&lt;li&gt;malloc的返回值是一个指针，指向一段可用内存的起始地址&lt;/li&gt;
&lt;li&gt;多次调用malloc所分配的地址不能有重叠部分，除非某次malloc所分配的地址被释放掉&lt;/li&gt;
&lt;li&gt;malloc应该尽快完成内存分配并返回（不能使用&lt;a href=&quot;http://en.wikipedia.org/wiki/NP-hard&quot;&gt;NP-hard&lt;/a&gt;的内存分配算法）&lt;/li&gt;
&lt;li&gt;实现malloc时应同时实现内存大小调整和内存释放函数（即realloc和free）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于malloc更多的说明可以在命令行中键入以下命令查看：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;man malloc
&lt;/pre&gt;
&lt;h1 id=&quot;2-预备知识&quot;&gt;2 预备知识&lt;/h1&gt;
&lt;p&gt;在实现malloc之前，需要先解释一些Linux系统内存相关的知识。&lt;/p&gt;
&lt;h2 id=&quot;21-linux内存管理&quot;&gt;2.1 Linux内存管理&lt;/h2&gt;
&lt;h3 id=&quot;211-虚拟内存地址与物理内存地址&quot;&gt;2.1.1 虚拟内存地址与物理内存地址&lt;/h3&gt;
&lt;p&gt;为了简单，现代操作系统在处理内存地址时，普遍采用虚拟内存地址技术。即在汇编程序（或机器语言）层面，当涉及内存地址时，都是使用虚拟内存地址。采用这种技术时，每个进程仿佛自己独享一片$2^N$字节的内存，其中$N$是机器位数。例如在64位CPU和64位操作系统下，每个进程的虚拟地址空间为$2^{64}$Byte。&lt;/p&gt;
&lt;p&gt;这种虚拟地址空间的作用主要是简化程序的编写及方便操作系统对进程间内存的隔离管理，真实中的进程不太可能（也用不到）如此大的内存空间，实际能用到的内存取决于物理内存大小。&lt;/p&gt;
&lt;p&gt;由于在机器语言层面都是采用虚拟地址，当实际的机器码程序涉及到内存操作时，需要根据当前进程运行的实际上下文将虚拟地址转换为物理内存地址，才能实现对真实内存数据的操作。这个转换一般由一个叫&lt;a href=&quot;http://en.wikipedia.org/wiki/Memory_management_unit&quot;&gt;MMU&lt;/a&gt;（Memory Management Unit）的硬件完成。&lt;/p&gt;
&lt;h3 id=&quot;212-页与地址构成&quot;&gt;2.1.2 页与地址构成&lt;/h3&gt;
&lt;p&gt;在现代操作系统中，不论是虚拟内存还是物理内存，都不是以字节为单位进行管理的，而是以页（Page）为单位。一个内存页是一段固定大小的连续内存地址的总称，具体到Linux中，典型的内存页大小为4096Byte（4K）。&lt;/p&gt;
&lt;p&gt;所以内存地址可以分为页号和页内偏移量。下面以64位机器，4G物理内存，4K页大小为例，虚拟内存地址和物理内存地址的组成如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog-codinglabs-org.qiniudn.com/image/a-malloc-tutorial-01.png&quot; alt=&quot;内存地址构成&quot;&gt;&lt;/p&gt;
&lt;p&gt;上面是虚拟内存地址，下面是物理内存地址。由于页大小都是4K，所以页内便宜都是用低12位表示，而剩下的高地址表示页号。&lt;/p&gt;
&lt;p&gt;MMU映射单位并不是字节，而是页，这个映射通过查一个常驻内存的数据结构&lt;a href=&quot;http://en.wikipedia.org/wiki/Page_table&quot;&gt;页表&lt;/a&gt;来实现。现在计算机具体的内存地址映射比较复杂，为了加快速度会引入一系列缓存和优化，例如&lt;a href=&quot;http://en.wikipedia.org/wiki/Translation_lookaside_buffer&quot;&gt;TLB&lt;/a&gt;等机制。下面给出一个经过简化的内存地址翻译示意图，虽然经过了简化，但是基本原理与现代计算机真实的情况的一致的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog-codinglabs-org.qiniudn.com/image/a-malloc-tutorial-02.png&quot; alt=&quot;内存地址翻译&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;213-内存页与磁盘页&quot;&gt;2.1.3 内存页与磁盘页&lt;/h3&gt;
&lt;p&gt;我们知道一般将内存看做磁盘的的缓存，有时MMU在工作时，会发现页表表明某个内存页不在物理内存中，此时会触发一个缺页异常（Page Fault），此时系统会到磁盘中相应的地方将磁盘页载入到内存中，然后重新执行由于缺页而失败的机器指令。关于这部分，因为可以看做对malloc实现是透明的，所以不再详细讲述，有兴趣的可以参考《深入理解计算机系统》相关章节。&lt;/p&gt;
&lt;p&gt;最后附上一张在维基百科找到的更加符合真实地址翻译的流程供大家参考，这张图加入了TLB和缺页异常的流程（&lt;a href=&quot;http://en.wikipedia.org/wiki/Page_table&quot;&gt;图片来源页&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog-codinglabs-org.qiniudn.com/image/a-malloc-tutorial-03.png&quot; alt=&quot;较为完整的地址翻译流程&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;22-linux进程级内存管理&quot;&gt;2.2 Linux进程级内存管理&lt;/h2&gt;
&lt;h3 id=&quot;221-内存排布&quot;&gt;2.2.1 内存排布&lt;/h3&gt;
&lt;p&gt;明白了虚拟内存和物理内存的关系及相关的映射机制，下面看一下具体在一个进程内是如何排布内存的。&lt;/p&gt;
&lt;p&gt;以Linux 64位系统为例。理论上，64bit内存地址可用空间为0x0000000000000000 ~ 0xFFFFFFFFFFFFFFFF，这是个相当庞大的空间，Linux实际上只用了其中一小部分（256T）。&lt;/p&gt;
&lt;p&gt;根据&lt;a href=&quot;https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt&quot;&gt;Linux内核相关文档&lt;/a&gt;描述，Linux64位操作系统仅使用低47位，高17位做扩展（只能是全0或全1）。所以，实际用到的地址为空间为0x0000000000000000 ~ 0x00007FFFFFFFFFFF和0xFFFF800000000000 ~ 0xFFFFFFFFFFFFFFFF，其中前面为用户空间（User Space），后者为内核空间（Kernel Space）。图示如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog-codinglabs-org.qiniudn.com/image/a-malloc-tutorial-04.png&quot; alt=&quot;Linux进程地址排布&quot;&gt;&lt;/p&gt;
&lt;p&gt;对用户来说，主要关注的空间是User Space。将User Space放大后，可以看到里面主要分为如下几段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code：这是整个用户空间的最低地址部分，存放的是指令（也就是程序所编译成的可执行机器码）&lt;/li&gt;
&lt;li&gt;Data：这里存放的是初始化过的全局变量&lt;/li&gt;
&lt;li&gt;BSS：这里存放的是未初始化的全局变量&lt;/li&gt;
&lt;li&gt;Heap：堆，这是我们本文重点关注的地方，堆自低地址向高地址增长，后面要讲到的brk相关的系统调用就是从这里分配内存&lt;/li&gt;
&lt;li&gt;Mapping Area：这里是与mmap系统调用相关的区域。大多数实际的malloc实现会考虑通过mmap分配较大块的内存区域，本文不讨论这种情况。这个区域自高地址向低地址增长&lt;/li&gt;
&lt;li&gt;Stack：这是栈区域，自高地址向低地址增长&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面我们主要关注Heap区域的操作。对整个Linux内存排布有兴趣的同学可以参考其它资料。&lt;/p&gt;
&lt;h3 id=&quot;222-heap内存模型&quot;&gt;2.2.2 Heap内存模型&lt;/h3&gt;
&lt;p&gt;一般来说，malloc所申请的内存主要从Heap区域分配（本文不考虑通过mmap申请大块内存的情况）。&lt;/p&gt;
&lt;p&gt;由上文知道，进程所面对的虚拟内存地址空间，只有按页映射到物理内存地址，才能真正使用。受物理存储容量限制，整个堆虚拟内存空间不可能全部映射到实际的物理内存。Linux对堆的管理示意如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog-codinglabs-org.qiniudn.com/image/a-malloc-tutorial-05.png&quot; alt=&quot;Linux进程堆管理&quot;&gt;&lt;/p&gt;
&lt;p&gt;Linux维护一个break指针，这个指针指向堆空间的某个地址。从堆起始地址到break之间的地址空间为映射好的，可以供进程访问；而从break往上，是未映射的地址空间，如果访问这段空间则程序会报错。&lt;/p&gt;
&lt;h3 id=&quot;223-brk与sbrk&quot;&gt;2.2.3 brk与sbrk&lt;/h3&gt;
&lt;p&gt;由上文知道，要增加一个进程实际的可用堆大小，就需要将break指针向高地址移动。Linux通过brk和sbrk系统调用操作break指针。两个系统调用的原型如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;int brk(void *addr);
void *sbrk(intptr_t increment);
&lt;/pre&gt;
&lt;p&gt;brk将break指针直接设置为某个地址，而sbrk将break从当前位置移动increment所指定的增量。brk在执行成功时返回0，否则返回-1并设置errno为ENOMEM；sbrk成功时返回break移动之前所指向的地址，否则返回(void *)-1。&lt;/p&gt;
&lt;p&gt;一个小技巧是，如果将increment设置为0，则可以获得当前break的地址。&lt;/p&gt;
&lt;p&gt;另外需要注意的是，由于Linux是按页进行内存映射的，所以如果break被设置为没有按页大小对齐，则系统实际上会在最后映射一个完整的页，从而实际已映射的内存空间比break指向的地方要大一些。但是使用break之后的地址是很危险的（尽管也许break之后确实有一小块可用内存地址）。&lt;/p&gt;
&lt;h3 id=&quot;224-资源限制与rlimit&quot;&gt;2.2.4 资源限制与rlimit&lt;/h3&gt;
&lt;p&gt;系统对每一个进程所分配的资源不是无限的，包括可映射的内存空间，因此每个进程有一个rlimit表示当前进程可用的资源上限。这个限制可以通过getrlimit系统调用得到，下面代码获取当前进程虚拟内存空间的rlimit：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;int main() {
    struct rlimit *limit = (struct rlimit *)malloc(sizeof(struct rlimit));
    getrlimit(RLIMIT_AS, limit);
    printf(&quot;soft limit: %ld, hard limit: %ld\n&quot;, limit-&gt;rlim_cur, limit-&gt;rlim_max);
}
&lt;/pre&gt;
&lt;p&gt;其中rlimit是一个结构体：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;struct rlimit {
    rlim_t rlim_cur;  /* Soft limit */
    rlim_t rlim_max;  /* Hard limit (ceiling for rlim_cur) */
};
&lt;/pre&gt;
&lt;p&gt;每种资源有软限制和硬限制，并且可以通过setrlimit对rlimit进行有条件设置。其中硬限制作为软限制的上限，非特权进程只能设置软限制，且不能超过硬限制。&lt;/p&gt;
&lt;h1 id=&quot;3-实现malloc&quot;&gt;3 实现malloc&lt;/h1&gt;
&lt;h2 id=&quot;31-玩具实现&quot;&gt;3.1 玩具实现&lt;/h2&gt;
&lt;p&gt;在正式开始讨论malloc的实现前，我们可以利用上述知识实现一个简单但几乎没法用于真实的玩具malloc，权当对上面知识的复习：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;/* 一个玩具malloc */
#include &lt;sys/types.h&gt;
#include &lt;unistd.h&gt;
void *malloc(size_t size)
{
    void *p;
    p = sbrk(0);
    if (sbrk(size) == (void *)-1)
        return NULL;
    return p;
}
&lt;/pre&gt;
&lt;p&gt;这个malloc每次都在当前break的基础上增加size所指定的字节数，并将之前break的地址返回。这个malloc由于对所分配的内存缺乏记录，不便于内存释放，所以无法用于真实场景。&lt;/p&gt;
&lt;h2 id=&quot;32-正式实现&quot;&gt;3.2 正式实现&lt;/h2&gt;
&lt;p&gt;下面严肃点讨论malloc的实现方案。&lt;/p&gt;
&lt;h3 id=&quot;321-数据结构&quot;&gt;3.2.1 数据结构&lt;/h3&gt;
&lt;p&gt;首先我们要确定所采用的数据结构。一个简单可行方案是将堆内存空间以块（Block）的形式组织起来，每个块由meta区和数据区组成，meta区记录数据块的元信息（数据区大小、空闲标志位、指针等等），数据区是真实分配的内存区域，并且数据区的第一个字节地址即为malloc返回的地址。&lt;/p&gt;
&lt;p&gt;可以用如下结构体定义一个block：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;typedef struct s_block *t_block;
struct s_block {
    size_t size;  /* 数据区大小 */
    t_block next; /* 指向下个块的指针 */
    int free;     /* 是否是空闲块 */
    int padding;  /* 填充4字节，保证meta块长度为8的倍数 */
    char data[1]  /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */
};
&lt;/pre&gt;
&lt;p&gt;由于我们只考虑64位机器，为了方便，我们在结构体最后填充一个int，使得结构体本身的长度为8的倍数，以便内存对齐。示意图如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog-codinglabs-org.qiniudn.com/image/a-malloc-tutorial-06.png&quot; alt=&quot;Block结构&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;322-寻找合适的block&quot;&gt;3.2.2 寻找合适的block&lt;/h3&gt;
&lt;p&gt;现在考虑如何在block链中查找合适的block。一般来说有两种查找算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;First fit&lt;/strong&gt;：从头开始，使用第一个数据区大小大于要求size的块所谓此次分配的块&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best fit&lt;/strong&gt;：从头开始，遍历所有块，使用数据区大小大于size且差值最小的块作为此次分配的块&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两种方法各有千秋，best fit具有较高的内存使用率（payload较高），而first fit具有更好的运行效率。这里我们采用first fit算法。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;/* First fit */
t_block find_block(t_block *last, size_t size) {
    t_block b = first_block;
    while(b &amp;&amp; !(b-&gt;free &amp;&amp; b-&gt;size &gt;= size)) {
        *last = b;
        b = b-&gt;next;
    }
    return b;
}
&lt;/pre&gt;
&lt;p&gt;find_block从frist_block开始，查找第一个符合要求的block并返回block起始地址，如果找不到这返回NULL。这里在遍历时会更新一个叫last的指针，这个指针始终指向当前遍历的block。这是为了如果找不到合适的block而开辟新block使用的，具体会在接下来的一节用到。&lt;/p&gt;
&lt;h3 id=&quot;323-开辟新的block&quot;&gt;3.2.3 开辟新的block&lt;/h3&gt;
&lt;p&gt;如果现有block都不能满足size的要求，则需要在链表最后开辟一个新的block。这里关键是如何只使用sbrk创建一个struct：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;#define BLOCK_SIZE 24 /* 由于存在虚拟的data字段，sizeof不能正确计算meta长度，这里手工设置 */

t_block extend_heap(t_block last, size_t s) {
    t_block b;
    b = sbrk(0);
    if(sbrk(BLOCK_SIZE + s) == (void *)-1)
        return NULL;
    b-&gt;size = s;
    b-&gt;next = NULL;
    if(last)
        last-&gt;next = b;
    b-&gt;free = 0;
    return b;
}
&lt;/pre&gt;
&lt;h3 id=&quot;324-分裂block&quot;&gt;3.2.4 分裂block&lt;/h3&gt;
&lt;p&gt;First fit有一个比较致命的缺点，就是可能会让很小的size占据很大的一块block，此时，为了提高payload，应该在剩余数据区足够大的情况下，将其分裂为一个新的block，示意如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog-codinglabs-org.qiniudn.com/image/a-malloc-tutorial-07.png&quot; alt=&quot;分裂block&quot;&gt;&lt;/p&gt;
&lt;p&gt;实现代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;void split_block(t_block b, size_t s) {
    t_block new;
    new = b-&gt;data + s;
    new-&gt;size = b-&gt;size - s - BLOCK_SIZE ;
    new-&gt;next = b-&gt;next;
    new-&gt;free = 1;
    b-&gt;size = s;
    b-&gt;next = new;
}
&lt;/pre&gt;
&lt;h3 id=&quot;325-malloc的实现&quot;&gt;3.2.5 malloc的实现&lt;/h3&gt;
&lt;p&gt;有了上面的代码，我们可以利用它们整合成一个简单但初步可用的malloc。注意首先我们要定义个block链表的头first_block，初始化为NULL；另外，我们需要剩余空间至少有BLOCK_SIZE + 8才执行分裂操作。&lt;/p&gt;
&lt;p&gt;由于我们希望malloc分配的数据区是按8字节对齐，所以在size不为8的倍数时，我们需要将size调整为大于size的最小的8的倍数：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;size_t align8(size_t s) {
    if(s &amp; 0x7 == 0)
        return s;
    return ((s &gt;&gt; 3) + 1) &lt;&lt; 3;
}
&lt;/pre&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;#define BLOCK_SIZE 24
void *first_block=NULL;

/* other functions... */

void *malloc(size_t size) {
    t_block b, last;
    size_t s;
    /* 对齐地址 */
    s = align8(size);
    if(first_block) {
        /* 查找合适的block */
        last = first_block;
        b = find_block(&amp;last, s);
        if(b) {
            /* 如果可以，则分裂 */
            if ((b-&gt;size - s) &gt;= ( BLOCK_SIZE + 8))
                split_block(b, s);
            b-&gt;free = 0;
        } else {
            /* 没有合适的block，开辟一个新的 */
            b = extend_heap(last, s);
            if(!b)
                return NULL;
        }
    } else {
        b = extend_heap(NULL, s);
        if(!b)
            return NULL;
        first_block = b;
    }
    return b-&gt;data;
}
&lt;/pre&gt;
&lt;h3 id=&quot;326-calloc的实现&quot;&gt;3.2.6 calloc的实现&lt;/h3&gt;
&lt;p&gt;有了malloc，实现calloc只要两步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;malloc一段内存&lt;/li&gt;
&lt;li&gt;将数据区内容置为0&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;由于我们的数据区是按8字节对齐的，所以为了提高效率，我们可以每8字节一组置0，而不是一个一个字节设置。我们可以通过新建一个size_t指针，将内存区域强制看做size_t类型来实现。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;void *calloc(size_t number, size_t size) {
    size_t *new;
    size_t s8, i;
    new = malloc(number * size);
    if(new) {
        s8 = align8(number * size) &gt;&gt; 3;
        for(i = 0; i &lt; s8; i++)
            new[i] = 0;
    }
    return new;
}
&lt;/pre&gt;
&lt;h3 id=&quot;327-free的实现&quot;&gt;3.2.7 free的实现&lt;/h3&gt;
&lt;p&gt;free的实现并不像看上去那么简单，这里我们要解决两个关键问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如何验证所传入的地址是有效地址，即确实是通过malloc方式分配的数据区首地址&lt;/li&gt;
&lt;li&gt;如何解决碎片问题&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;首先我们要保证传入free的地址是有效的，这个有效包括两方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;地址应该在之前malloc所分配的区域内，即在first_block和当前break指针范围内&lt;/li&gt;
&lt;li&gt;这个地址确实是之前通过我们自己的malloc分配的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一个问题比较好解决，只要进行地址比较就可以了，关键是第二个问题。这里有两种解决方案：一是在结构体内埋一个magic number字段，free之前通过相对偏移检查特定位置的值是否为我们设置的magic number，另一种方法是在结构体内增加一个magic pointer，这个指针指向数据区的第一个字节（也就是在合法时free时传入的地址），我们在free前检查magic pointer是否指向参数所指地址。这里我们采用第二种方案：&lt;/p&gt;
&lt;p&gt;首先我们在结构体中增加magic pointer（同时要修改BLOCK_SIZE）：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;typedef struct s_block *t_block;
struct s_block {
    size_t size;  /* 数据区大小 */
    t_block next; /* 指向下个块的指针 */
    int free;     /* 是否是空闲块 */
    int padding;  /* 填充4字节，保证meta块长度为8的倍数 */
    void *ptr;    /* Magic pointer，指向data */
    char data[1]  /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */
};
&lt;/pre&gt;
&lt;p&gt;然后我们定义检查地址合法性的函数：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;t_block get_block(void *p) {
    char *tmp;  
    tmp = p;
    return (p = tmp -= BLOCK_SIZE);
}

int valid_addr(void *p) {
    if(first_block) {
        if(p &gt; first_block &amp;&amp; p &lt; sbrk(0)) {
            return p == (get_block(p))-&gt;ptr;
        }
    }
    return 0;
}
&lt;/pre&gt;
&lt;p&gt;当多次malloc和free后，整个内存池可能会产生很多碎片block，这些block很小，经常无法使用，甚至出现许多碎片连在一起，虽然总体能满足某此malloc要求，但是由于分割成了多个小block而无法fit，这就是碎片问题。&lt;/p&gt;
&lt;p&gt;一个简单的解决方式时当free某个block时，如果发现它相邻的block也是free的，则将block和相邻block合并。为了满足这个实现，需要将s_block改为双向链表。修改后的block结构如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;typedef struct s_block *t_block;
struct s_block {
    size_t size;  /* 数据区大小 */
    t_block prev; /* 指向上个块的指针 */
    t_block next; /* 指向下个块的指针 */
    int free;     /* 是否是空闲块 */
    int padding;  /* 填充4字节，保证meta块长度为8的倍数 */
    void *ptr;    /* Magic pointer，指向data */
    char data[1]  /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */
};
&lt;/pre&gt;
&lt;p&gt;合并方法如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;t_block fusion(t_block b) {
    if (b-&gt;next &amp;&amp; b-&gt;next-&gt;free) {
        b-&gt;size += BLOCK_SIZE + b-&gt;next-&gt;size;
        b-&gt;next = b-&gt;next-&gt;next;
        if(b-&gt;next)
            b-&gt;next-&gt;prev = b;
    }
    return b;
}
&lt;/pre&gt;
&lt;p&gt;有了上述方法，free的实现思路就比较清晰了：首先检查参数地址的合法性，如果不合法则不做任何事；否则，将此block的free标为1，并且在可以的情况下与后面的block进行合并。如果当前是最后一个block，则回退break指针释放进程内存，如果当前block是最后一个block，则回退break指针并设置first_block为NULL。实现如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;void free(void *p) {
    t_block b;
    if(valid_addr(p)) {
        b = get_block(p);
        b-&gt;free = 1;
        if(b-&gt;prev &amp;&amp; b-&gt;prev-&gt;free)
            b = fusion(b-&gt;prev);
        if(b-&gt;next)
            fusion(b);
        else {
            if(b-&gt;prev)
                b-&gt;prev-&gt;prev = NULL;
            else
                first_block = NULL;
            brk(b);
        }
    }
}
&lt;/pre&gt;
&lt;h3 id=&quot;328-realloc的实现&quot;&gt;3.2.8 realloc的实现&lt;/h3&gt;
&lt;p&gt;为了实现realloc，我们首先要实现一个内存复制方法。如同calloc一样，为了效率，我们以8字节为单位进行复制：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;void copy_block(t_block src, t_block dst) {
    size_t *sdata, *ddata;
    size_t i;
    sdata = src-&gt;ptr;
    ddata = dst-&gt;ptr;
    for(i = 0; (i * 8) &lt; src-&gt;size &amp;&amp; (i * 8) &lt; dst-&gt;size; i++)
        ddata[i] = sdata[i];
}
&lt;/pre&gt;
&lt;p&gt;然后我们开始实现realloc。一个简单（但是低效）的方法是malloc一段内存，然后将数据复制过去。但是我们可以做的更高效，具体可以考虑以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果当前block的数据区大于等于realloc所要求的size，则不做任何操作&lt;/li&gt;
&lt;li&gt;如果新的size变小了，考虑split&lt;/li&gt;
&lt;li&gt;如果当前block的数据区不能满足size，但是其后继block是free的，并且合并后可以满足，则考虑做合并&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面是realloc的实现：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-c&quot;&gt;void *realloc(void *p, size_t size) {
    size_t s;
    t_block b, new;
    void *newp;
    if (!p)
        /* 根据标准库文档，当p传入NULL时，相当于调用malloc */
        return malloc(size);
    if(valid_addr(p)) {
        s = align8(size);
        b = get_block(p);
        if(b-&gt;size &gt;= s) {
            if(b-&gt;size - s &gt;= (BLOCK_SIZE + 8))
                split_block(b,s);
        } else {
            /* 看是否可进行合并 */
            if(b-&gt;next &amp;&amp; b-&gt;next-&gt;free
                    &amp;&amp; (b-&gt;size + BLOCK_SIZE + b-&gt;next-&gt;size) &gt;= s) {
                fusion(b);
                if(b-&gt;size - s &gt;= (BLOCK_SIZE + 8))
                    split_block(b, s);
            } else {
                /* 新malloc */
                newp = malloc (s);
                if (!newp)
                    return NULL;
                new = get_block(newp);
                copy_block(b, new);
                free(p);
                return(newp);
            }
        }
        return (p);
    }
    return NULL;
}
&lt;/pre&gt;
&lt;h2 id=&quot;33-遗留问题和优化&quot;&gt;3.3 遗留问题和优化&lt;/h2&gt;
&lt;p&gt;以上是一个较为简陋，但是初步可用的malloc实现。还有很多遗留的可能优化点，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同时兼容32位和64位系统&lt;/li&gt;
&lt;li&gt;在分配较大快内存时，考虑使用mmap而非sbrk，这通常更高效&lt;/li&gt;
&lt;li&gt;可以考虑维护多个链表而非单个，每个链表中的block大小均为一个范围内，例如8字节链表、16字节链表、24-32字节链表等等。此时可以根据size到对应链表中做分配，可以有效减少碎片，并提高查询block的速度&lt;/li&gt;
&lt;li&gt;可以考虑链表中只存放free的block，而不存放已分配的block，可以减少查找block的次数，提高效率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;还有很多可能的优化，这里不一一赘述。下面附上一些参考文献，有兴趣的同学可以更深入研究。&lt;/p&gt;
&lt;h1 id=&quot;4-其它参考&quot;&gt;4 其它参考&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;这篇文章大量参考了&lt;a href=&quot;http://www.inf.udec.cl/~leo/Malloc_tutorial.pdf&quot;&gt;A malloc Tutorial&lt;/a&gt;，其中一些图片和代码直接引用了文中的内容，这里特别指出&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://csapp.cs.cmu.edu/&quot;&gt;Computer Systems: A Programmer&#39;s Perspective, 2/E&lt;/a&gt;一书有许多值得参考的地方&lt;/li&gt;
&lt;li&gt;关于Linux的虚拟内存模型，&lt;a href=&quot;http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory/&quot;&gt;Anatomy of a Program in Memory&lt;/a&gt;是很好的参考资料，另外作者还有一篇&lt;a href=&quot;http://duartes.org/gustavo/blog/post/how-the-kernel-manages-your-memory/&quot;&gt;How the Kernel Manages Your Memory&lt;/a&gt;对于Linux内核中虚拟内存管理的部分有很好的讲解&lt;/li&gt;
&lt;li&gt;对于真实世界的malloc实现，可以参考&lt;a href=&quot;http://repo.or.cz/w/glibc.git/blob/HEAD:/malloc/malloc.c&quot;&gt;glibc的实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;本文写作过程中大量参考了&lt;a href=&quot;http://www.wikipedia.org/&quot;&gt;维基百科&lt;/a&gt;，再次感谢这个伟大的网站，并且呼吁大家在手头允许的情况下可以适当捐助维基百科，帮助这个造福人类的系统运行下去&lt;/li&gt;
&lt;/ol&gt;
</description> 
        </item> 
        
        <item> 
            <title>生成特定分布随机数的方法</title> 
            <link>http://blog.codinglabs.org/articles/methods-for-generating-random-number-distributions.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/methods-for-generating-random-number-distributions.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Sat, 14 Jun 2014 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;生成随机数是程序设计里常见的需求。一般的编程语言都会自带一个随机数生成函数，用于生成服从均匀分布的随机数。不过有时需要生成服从其它分布的随机数，例如高斯分布或指数分布等。有些编程语言已经有比较完善的实现，例如Python的NumPy。这篇文章介绍如何通过均匀分布随机数生成函数生成符合特定概率分布的随机数，主要介绍Inverse Ttransform和Acceptance-Rejection两种基础算法以及一些相关的衍生方法。下文我们均假设已经拥有一个可以生成0到1之间均匀分布的随机数生成函数，关于如何生成均匀分布等更底层的随机数生成理论，请参考其它资料，本文不做讨论。&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;h1 id=&quot;基础算法&quot;&gt;基础算法&lt;/h1&gt;
&lt;h2 id=&quot;inverse-transform-method&quot;&gt;Inverse Transform Method&lt;/h2&gt;
&lt;p&gt;最简单的生成算法是Inverse Transform Method（下文简称ITM）。如果我们可以给出概率分布的累积分布函数（下文简称CDF）及其逆函数的解析表达式，则可以非常简单便捷的生成指定分布随机数。&lt;/p&gt;
&lt;h3 id=&quot;itm算法描述&quot;&gt;ITM算法描述&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;生成一个服从均匀分布的随机数\(U \sim Uni(0,1)\)&lt;/li&gt;
&lt;li&gt;设\(F(X)\)为指定分布的CDF，\(F^{-1}(Y)\)是其逆函数。返回\(X=F^{-1}(U)\)作为结果&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;itm算法说明&quot;&gt;ITM算法说明&lt;/h3&gt;
&lt;p&gt;这是一个非常简洁高效的算法，下面说明其原理及正确性。&lt;/p&gt;
&lt;p&gt;我们通过图示可以更直观的明白算法的原理。下图是某概率分布的CDF：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/methods-for-generating-random-number-distributions/inverse-transformation.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;我们从横轴上标注两点\(x_a\)和\(x_b\)，其CDF值分别为\(F(x_a)\)和\(F(x_b)\)。&lt;/p&gt;
&lt;p&gt;由于U服从0到1之间的均匀分布，因此对于一次U的取样，U落入\(F(x_a)\)和\(F(x_b)\)之间的概率为:&lt;/p&gt;
&lt;p&gt;\[P(U \in (F(x_a),F(x_b))) = F(x_b) - F(x_a)\]&lt;/p&gt;
&lt;p&gt;而由于CDF都是单调非减函数，因此这个概率同时等于\(X\)落入\(x_a\)和\(x_b\)之间的概率，即：&lt;/p&gt;
&lt;p&gt;\[P(U \in (F(x_a),F(x_b))) = P(F^{-1}(U) \in (F^{-1}(F(x_a)),F^{-1}(F(x_b)))) = P(X \in (x_a,x_b)) = F(x_b) - F(x_a)\]&lt;/p&gt;
&lt;p&gt;而根据CDF的定义，这刚好说明\(X\)服从以\(F(x)\)为CDF的分布，因此我们的生成算法是正确的。&lt;/p&gt;
&lt;h3 id=&quot;itm实现示例&quot;&gt;ITM实现示例&lt;/h3&gt;
&lt;p&gt;下面以&lt;a href=&quot;http://en.wikipedia.org/wiki/Exponential_distribution&quot;&gt;指数分布&lt;/a&gt;为例说明如何应用ITM。&lt;/p&gt;
&lt;p&gt;首先我们需要求解CDF的逆函数。我们知道指数分布的CDF为&lt;/p&gt;
&lt;p&gt;\[F(X)=1-e^{-\lambda X}\]&lt;/p&gt;
&lt;p&gt;通过简单的代数运算，可以得到其逆函数为&lt;/p&gt;
&lt;p&gt;\[F^{-1}(Y)=-\frac{1}{\lambda}\ln(1-Y)\]&lt;/p&gt;
&lt;p&gt;由于\(U\)服从从0到1的均匀分布蕴含着\(1-U\)服从同样的分布，因此在实际实现时可以用\(Y\)代替\(1-Y\)，得到：&lt;/p&gt;
&lt;p&gt;\[F^{-1}(Y)=-\frac{1}{\lambda}\ln(Y)\]&lt;/p&gt;
&lt;p&gt;下面给出一个Python的实现示例程序。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-python&quot;&gt;import random
import math
def exponential_rand(lam):
    if lam &lt;= 0:
        return -1
    U = random.uniform(0.0, 1.0)
    return (-1.0 / lam) * math.log(U)
&lt;/pre&gt;
&lt;h2 id=&quot;acceptance-rejection-method&quot;&gt;Acceptance-Rejection Method&lt;/h2&gt;
&lt;p&gt;一般来说ITM是一种很好的算法，简单且高效，如果可以使用的话，是第一选择。但是ITM有自身的局限性，就是要求必须能给出CDF逆函数的解析表达式，有些时候要做到这点比较困难，这限制了ITM的适用范围。&lt;/p&gt;
&lt;p&gt;当无法给出CDF逆函数的解析表达式时，Acceptance-Rejection Method（下文简称ARM）是另外的选择。ARM的适用范围比ITM要大，只要给出概率密度函数（下文简称PDF）的解析表达式即可，而大多数常用分布的PDF是可以查到的。&lt;/p&gt;
&lt;h3 id=&quot;arm算法描述&quot;&gt;ARM算法描述&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;设PDF为\(f(x)\)。首先生成一个均匀分布随机数\(X \sim Uni(x_{min},x_{max})\)&lt;/li&gt;
&lt;li&gt;独立的生成另一个均匀分布随机数\(Y \sim Uni(y_{min},y_{max})\)&lt;/li&gt;
&lt;li&gt;如果\(Y \leq f(X)\)，则返回\(X\)，否则回到第1步&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;arm算法说明&quot;&gt;ARM算法说明&lt;/h3&gt;
&lt;p&gt;通过一幅图可以清楚的看到ARM的工作原理。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/methods-for-generating-random-number-distributions/accept-reject.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;ARM本质上是一种模拟方法，而非直接数学方法。它每次生成新的随机数后，通过另一个随机数来保证其被接受概率服从指定的PDF。&lt;/p&gt;
&lt;p&gt;显然ARM从效率上不如ITM，但是其适应性更广，在无法得到CDF的逆函数时，ARM是不错的选择。&lt;/p&gt;
&lt;h3 id=&quot;arm实现示例&quot;&gt;ARM实现示例&lt;/h3&gt;
&lt;p&gt;下面使用ARM实现一个能产生&lt;a href=&quot;http://en.wikipedia.org/wiki/Gauss_distribution&quot;&gt;标准正态分布&lt;/a&gt;的随机数生成函数。&lt;/p&gt;
&lt;p&gt;首先我们要得到标准正态分布的PDF，其数学表示为：&lt;/p&gt;
&lt;p&gt;\[f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\]&lt;/p&gt;
&lt;p&gt;为了方便，这里我会直接使用&lt;a href=&quot;http://www.scipy.org/&quot;&gt;SciPy&lt;/a&gt;来计算其PDF。&lt;/p&gt;
&lt;p&gt;程序如下。&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-python&quot;&gt;import random
import scipy.stats as ss

def standard_normal_rand():
    while True:
        X = random.uniform(-3.0,3.0)
        Y = random.uniform(0.0, 0.5)
        if Y &lt; ss.norm.pdf(X):
            return X
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：标准正态分布的x取值范围从理论上说是\((-\infty,\infty)\)，但是当离开均值点很远后，其概率密度可忽略不计。这里只取\((-3.0,3.0)\)，实际使用时可以根据具体需要扩大这个取值范围。&lt;/p&gt;
&lt;h1 id=&quot;衍生算法&quot;&gt;衍生算法&lt;/h1&gt;
&lt;h2 id=&quot;组合算法&quot;&gt;组合算法&lt;/h2&gt;
&lt;p&gt;当目标分布可以用其它分布经过四则运算表示时，可以使用组合算法生成对应随机数。&lt;/p&gt;
&lt;p&gt;最常见的就是某分布可以表示成多个独立同分布（下文简称IID）随机变量之和。例如二项分布可以表示成多个0-1分布之和，&lt;a href=&quot;http://en.wikipedia.org/wiki/Erlang_distribution&quot;&gt;Erlang分布&lt;/a&gt;可以由多个IID的指数分布得出。&lt;/p&gt;
&lt;p&gt;以Erlang分布为例说明如何生成这类随机数。&lt;/p&gt;
&lt;p&gt;设\(X_1,X_2,\cdots,X_k\)为服从0到1均匀分布的IID随机数，则\(-\frac{1}{\lambda}lnX_1,-\frac{1}{\lambda}lnX_2,\cdots,-\frac{1}{\lambda}lnX_k\)为服从指数分布的IID随机数，因此&lt;/p&gt;
&lt;p&gt;\[X=-\frac{1}{\lambda}lnX_1-\frac{1}{\lambda}lnX_2-\cdots-\frac{1}{\lambda}lnX_k=-\frac{1}{\lambda}ln\prod_{i=1}^k{X_i}\sim Erl(k,\lambda)\]&lt;/p&gt;
&lt;p&gt;所以生成Erlang分布随机数的算法如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;生成\(X_1,X_2,\cdots,X_k\sim Uni(0,1)\)&lt;/li&gt;
&lt;li&gt;返回\(-\frac{1}{\lambda}ln\prod_{i=1}^k{X_i}\)&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;这类分布的随机数生成算法很直观，就是先生成相关的n个IID随机数，然后带入简单求和公式或其它四则公式得出最终随机数。其数学理论基础是&lt;a href=&quot;http://en.wikipedia.org/wiki/Convolution&quot;&gt;卷积理论&lt;/a&gt;，稍微有些复杂，这里不再讨论，有兴趣的同学可以查阅相关资料。&lt;/p&gt;
&lt;h2 id=&quot;生成具有相关性的随机数&quot;&gt;生成具有相关性的随机数&lt;/h2&gt;
&lt;p&gt;现在考虑生成多维随机数，以最简单的二维随机数为例。&lt;/p&gt;
&lt;p&gt;如果两个维度的随机数是相互独立的，那么只要分别生成两个列就可以了。但是如果要求两列具有一定的相关系数，则需要做一些特殊处理。&lt;/p&gt;
&lt;p&gt;下列算法可以生成两列具有相关系数\(\rho\)的随机数。&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;生成IID随机变量\(X\)和\(Y\)&lt;/li&gt;
&lt;li&gt;计算\(X&#39;=\rho X+\sqrt{1-\rho^2}Y\)&lt;/li&gt;
&lt;li&gt;返回\((X,X&#39;)\)&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以这样验证其正确性：&lt;/p&gt;
&lt;p&gt;\[corr(X,X&#39;)=\rho corr(X,X)+\sqrt{1-\rho^2}corr(X,Y)=\rho\]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：\(corr(X,X)=1\)，\(corr(X,Y)=0\)。&lt;/p&gt;
&lt;p&gt;因此\(X\)和\(X&#39;\)确实具有相关系数\(\rho\)。&lt;/p&gt;
&lt;h1 id=&quot;更多参考&quot;&gt;更多参考&lt;/h1&gt;
&lt;p&gt;这篇文章讨论了生成指定分布随机数的基本方法。这篇文章只打算讨论基础方法，所以还有很多有趣的内容，本文没有深入的探讨。这里给出一些扩展阅读资料，供有兴趣的朋友深入学习。首先是一篇&lt;a href=&quot;http://ftp.arl.mil/random/random.pdf&quot;&gt;非常好的文档&lt;/a&gt;，这篇文章来自美国陆军实验室，对计算机生成指定分布随机数的方方面面进行了全面深入描述，是不可多得的好资料。&lt;/p&gt;
&lt;p&gt;在实现方面，可以参考&lt;a href=&quot;https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/distributions.c&quot;&gt;NumPy中关于random的实现&lt;/a&gt;以及我开发的&lt;a href=&quot;https://github.com/ericzhang-cn/random.js&quot;&gt;JavaScript实现&lt;/a&gt;。另外我做过一个&lt;a href=&quot;http://blog.codinglabs.org/demo/distributions.html&quot;&gt;不同概率分布的可视化页面&lt;/a&gt;，可以帮助你直观理解不同分布及PDF参数对分布的影响。&lt;/p&gt;
</description> 
        </item> 
        
        <item> 
            <title>2048-AI程序算法分析</title> 
            <link>http://blog.codinglabs.org/articles/2048-ai-analysis.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/2048-ai-analysis.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Fri, 04 Apr 2014 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;针对目前火爆的&lt;a href=&quot;http://gabrielecirulli.github.io/2048/&quot;&gt;2048&lt;/a&gt;游戏，&lt;a href=&quot;http://ov3y.github.io/2048-AI/&quot;&gt;有人实现了一个AI程序&lt;/a&gt;，可以以较大概率（高于90%）赢得游戏，并且&lt;a href=&quot;http://stackoverflow.com/questions/22342854/what-is-the-optimal-algorithm-for-the-game-2048&quot;&gt;作者在stackoverflow上简要介绍了AI的算法框架和实现思路&lt;/a&gt;。但是这个回答主要集中在启发函数的选取上，对AI用到的核心算法并没有仔细说明。这篇文章将主要分为两个部分，第一部分介绍其中用到的基础算法，即Minimax和Alpha-beta剪枝；第二部分分析作者具体的实现。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/00.png&quot;/&gt;&lt;/p&gt;

&lt;h1 id=&quot;基础算法&quot;&gt;基础算法&lt;/h1&gt;
&lt;p&gt;2048本质上可以抽象成信息对称双人对弈模型（玩家向四个方向中的一个移动，然后计算机在某个空格中填入2或4）。这里“信息对称”是指在任一时刻对弈双方对格局的信息完全一致，移动策略仅依赖对接下来格局的推理。作者使用的核心算法为对弈模型中常用的带Alpha-beta剪枝的Minimax。这个算法也常被用于如国际象棋等信息对称对弈AI中。&lt;/p&gt;
&lt;h2 id=&quot;minimax&quot;&gt;Minimax&lt;/h2&gt;
&lt;p&gt;下面先介绍不带剪枝的Minimax。首先本文将通过一个简单的例子说明Minimax算法的思路和决策方式。&lt;/p&gt;
&lt;h3 id=&quot;问题&quot;&gt;问题&lt;/h3&gt;
&lt;p&gt;现在考虑这样一个游戏：有三个盘子A、B和C，每个盘子分别放有三张纸币。A放的是1、20、50；B放的是5、10、100；C放的是1、5、20。单位均为“元”。有甲、乙两人，两人均对三个盘子和上面放置的纸币有可以任意查看。游戏分三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;甲从三个盘子中选取一个。&lt;/li&gt;
&lt;li&gt;乙从甲选取的盘子中拿出两张纸币交给甲。&lt;/li&gt;
&lt;li&gt;甲从乙所给的两张纸币中选取一张，拿走。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中甲的目标是最后拿到的纸币面值尽量大，乙的目标是让甲最后拿到的纸币面值尽量小。&lt;/p&gt;
&lt;p&gt;下面用Minimax算法解决这个问题。&lt;/p&gt;
&lt;h3 id=&quot;基本思路&quot;&gt;基本思路&lt;/h3&gt;
&lt;p&gt;一般解决博弈类问题的自然想法是将格局组织成一棵树，树的每一个节点表示一种格局，而父子关系表示由父格局经过一步可以到达子格局。Minimax也不例外，它通过对以当前格局为根的格局树搜索来确定下一步的选择。而一切格局树搜索算法的核心都是对每个格局价值的评价。Minimax算法基于以下朴素思想确定格局价值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimax是一种悲观算法，即假设对手每一步都会将我方引入从当前看理论上价值最小的格局方向，即对手具有完美决策能力。因此我方的策略应该是选择那些对方所能达到的让我方最差情况中最好的，也就是让对方在完美决策下所对我造成的损失最小。&lt;/li&gt;
&lt;li&gt;Minimax不找理论最优解，因为理论最优解往往依赖于对手是否足够愚蠢，Minimax中我方完全掌握主动，如果对方每一步决策都是完美的，则我方可以达到预计的最小损失格局，如果对方没有走出完美决策，则我方可能达到比预计的最悲观情况更好的结局。总之我方就是要在最坏情况中选择最好的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的表述有些抽象，下面看具体示例。&lt;/p&gt;
&lt;h3 id=&quot;解题&quot;&gt;解题&lt;/h3&gt;
&lt;p&gt;下图是上述示例问题的格局树：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/01.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;注意，由于示例问题格局数非常少，我们可以给出完整的格局树。这种情况下我可以找到Minimax算法的全局最优解。而真实情况中，格局树非常庞大，即使是计算机也不可能给出完整的树，因此我们往往只搜索一定深度，这时只能找到局部最优解。&lt;/p&gt;
&lt;p&gt;我们从甲的角度考虑。其中正方形节点表示轮到我方（甲），而三角形表示轮到对方（乙）。经过三轮对弈后（我方-对方-我方），将进入终局。黄色叶结点表示所有可能的结局。从甲方看，由于最终的收益可以通过纸币的面值评价，我们自然可以用结局中甲方拿到的纸币面值表示终格局的价值。&lt;/p&gt;
&lt;p&gt;下面考虑倒数第二层节点，在这些节点上，轮到我方选择，所以我们应该引入可选择的最大价值格局，因此每个节点的价值为其子节点的最大值：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/02.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;这些轮到我方的节点叫做max节点，max节点的值是其子节点最大值。&lt;/p&gt;
&lt;p&gt;倒数第三层轮到对方选择，假设对方会尽力将局势引入让我方价值最小的格局，因此这些节点的价值取决于子节点的最小值。这些轮到对方的节点叫做min节点。&lt;/p&gt;
&lt;p&gt;最后，根节点是max节点，因此价值取决于叶子节点的最大值。最终完整赋值的格局树如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/03.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;总结一下Minimax算法的步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先确定最大搜索深度D，D可能达到终局，也可能是一个中间格局。&lt;/li&gt;
&lt;li&gt;在最大深度为D的格局树叶子节点上，使用预定义的价值评价函数对叶子节点价值进行评价。&lt;/li&gt;
&lt;li&gt;自底向上为非叶子节点赋值。其中max节点取子节点最大值，min节点取子节点最小值。&lt;/li&gt;
&lt;li&gt;每次轮到我方时（此时必处在格局树的某个max节点），选择价值等于此max节点价值的那个子节点路径。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在上面的例子中，根节点的价值为20，表示如果对方每一步都完美决策，则我方按照上述算法可最终拿到20元，这是我方在Minimax算法下最好的决策。格局转换路径如下图红色路径所示：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/04.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;对于真实问题中的Minimax，再次强调几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实问题一般无法构造出完整的格局树，所以需要确定一个最大深度D，每次最多从当前格局向下计算D层。&lt;/li&gt;
&lt;li&gt;因为上述原因，Minimax一般是寻找一个局部最优解而不是全局最优解，搜索深度越大越可能找到更好的解，但计算耗时会呈指数级膨胀。&lt;/li&gt;
&lt;li&gt;也是因为无法一次构造出完整的格局树，所以真实问题中Minimax一般是边对弈边计算局部格局树，而不是只计算一次，但已计算的中间结果可以缓存。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;alpha-beta剪枝&quot;&gt;Alpha-beta剪枝&lt;/h2&gt;
&lt;p&gt;简单的Minimax算法有一个很大的问题就是计算复杂性。由于所需搜索的节点数随最大深度呈指数膨胀，而算法的效果往往和深度相关，因此这极大限制了算法的效果。&lt;/p&gt;
&lt;p&gt;Alpha-beta剪枝是对Minimax的补充和改进。采用Alpha-beta剪枝后，我们可不必构造和搜索最大深度D内的所有节点，在构造过程中，如果发现当前格局再往下不能找到更好的解，我们就停止在这个格局及以下的搜索，也就是剪枝。&lt;/p&gt;
&lt;p&gt;Alpha-beta基于这样一种朴素的思想：时时刻刻记得当前已经知道的最好选择，如果从当前格局搜索下去，不可能找到比已知最优解更好的解，则停止这个格局分支的搜索（剪枝），回溯到父节点继续搜索。&lt;/p&gt;
&lt;p&gt;Alpha-beta算法可以看成变种的Minimax，基本方法是从根节点开始采用深度优先的方式构造格局树，在构造每个节点时，都会读取此节点的alpha和beta两个值，其中alpha表示搜索到当前节点时已知的最好选择的下界，而beta表示从这个节点往下搜索最坏结局的上界。由于我们假设对手会将局势引入最坏结局之一，因此当beta小于alpha时，表示从此处开始不论最终结局是哪一个，其上限价值也要低于已知的最优解，也就是说已经不可能此处向下找到更好的解，所以就会剪枝。&lt;/p&gt;
&lt;p&gt;下面同样以上述示例介绍Alpha-beta剪枝算法的工作原理。我们从根节点开始，详述使用Alpha-beta的每一个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根节点的alpha和beta分别被初始化为\(-\infty\)，和\(+\infty\)。&lt;/li&gt;
&lt;li&gt;深度优先搜索第一个孩子，不是叶子节点，所以alpha和beta继承自父节点，分别为\(-\infty\)，和\(+\infty\)&lt;/li&gt;
&lt;li&gt;搜索第三层的第一个孩子，同上。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;搜索第四层，到达叶子节点，采用评价函数得到此节点的评价值为1。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/05.png&quot;/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;此叶节点的父节点为max节点，因此更新其alpha值为1，表示此节点取值的下界为1。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;再看另外一个子节点，值为20，大于当前alpha值，因此将alpha值更新为20。&lt;/li&gt;
&lt;li&gt;此时第三层最左节点所有子树搜索完毕，作为max节点，更新其真实值为当前alpha值：20。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;由于其父节点（第二层最左节点）为min节点，因此更新其父节点beta值为20，表示这个节点取值最多为20。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/06.png&quot;/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;搜索第二层最左节点的第二个孩子及其子树，按上述逻辑，得到值为50（注意第二层最左节点的beta值要传递给孩子）。由于50大于20，不更新min节点的beta值。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/07.png&quot;/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;搜索第二层最左节点的第三个孩子。当看完第一个叶子节点后，发现第三个孩子的alpha=beta，此时表示这个节点下不会再有更好解，于是剪枝。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/08.png&quot;/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;继续搜索B分支，当搜索完B分支的第一个孩子后，发现此时B分支的alpha为20，beta为10。这表示B分支节点的最大取值不会超过10，而我们已经在A分支取到20，此时满足alpha大于等于beta的剪枝条件，因此将B剪枝。并将B分支的节点值设为10，注意，这个10不一定是这个节点的真实值，而只是上线，B节点的真实值可能是5，可能是1，可能是任何小于10的值。但是已经无所谓了，反正我们知道这个分支不会好过A分支，因此可以放弃了。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/09.png&quot;/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在C分支搜索时遇到了与B分支相同的情况。因此讲C分支剪枝。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/10.png&quot;/&gt;&lt;/p&gt;

&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此时搜索全部完毕，而我们也得到了这一步的策略：应该走A分支。&lt;/p&gt;
&lt;p&gt;可以看到相比普通Minimax要搜索18个叶子节点相比，这里只搜索了9个。采用Alpha-beta剪枝，可以在相同时间内加大Minimax的搜索深度，因此可以获得更好的效果。并且Alpha-beta的解和普通Minimax的解是一致的。&lt;/p&gt;
&lt;h1 id=&quot;针对2048游戏的实现&quot;&gt;针对2048游戏的实现&lt;/h1&gt;
&lt;p&gt;下面看一下ov3y同学针对2048实现的AI。程序的github在&lt;a href=&quot;https://github.com/ov3y/2048-AI&quot;&gt;这里&lt;/a&gt;，主要程序都在&lt;a href=&quot;https://github.com/ov3y/2048-AI/blob/master/js/ai.js&quot;&gt;ai.js&lt;/a&gt;中。&lt;/p&gt;
&lt;h2 id=&quot;建模&quot;&gt;建模&lt;/h2&gt;
&lt;p&gt;上面说过Minimax和Alpha-beta都是针对信息对称的轮流对弈问题，这里作者是这样抽象游戏的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我方：游戏玩家。每次可以选择上、下、左、右四个行棋策略中的一种（某些格局会少于四种，因为有些方向不可走）。行棋后方块按照既定逻辑移动及合并，格局转换完成。&lt;/li&gt;
&lt;li&gt;对方：计算机。在当前任意空格子里放置一个方块，方块的数值可以是2或4。放置新方块后，格局转换完成。&lt;/li&gt;
&lt;li&gt;胜利条件：出现某个方块的数值为“2048”。&lt;/li&gt;
&lt;li&gt;失败条件：格子全满，且无法向四个方向中任何一个方向移动（均不能触发合并）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如此2048游戏就被建模成一个信息对称的双人对弈问题。&lt;/p&gt;
&lt;h2 id=&quot;格局评价&quot;&gt;格局评价&lt;/h2&gt;
&lt;p&gt;作为算法的核心，如何评价当前格局的价值是重中之重。在2048中，除了终局外，中间格局并无非常明显的价值评价指标，因此需要用一些启发式的指标来评价格局。那些分数高的“好”格局是容易引向胜利的格局，而分低的“坏”格局是容易引向失败的格局。&lt;/p&gt;
&lt;p&gt;作者采用了如下几个启发式指标。&lt;/p&gt;
&lt;h3 id=&quot;单调性&quot;&gt;单调性&lt;/h3&gt;
&lt;p&gt;单调性指方块从左到右、从上到下均遵从递增或递减。一般来说，越单调的格局越好。下面是一个具有良好单调格局的例子：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/11.png&quot;/&gt;&lt;/p&gt;

&lt;h3 id=&quot;平滑性&quot;&gt;平滑性&lt;/h3&gt;
&lt;p&gt;平滑性是指每个方块与其直接相邻方块数值的差，其中差越小越平滑。例如2旁边是4就比2旁边是128平滑。一般认为越平滑的格局越好。下面是一个具有极端平滑性的例子：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/2048-ai-analysis/12.png&quot;/&gt;&lt;/p&gt;

&lt;h3 id=&quot;空格数&quot;&gt;空格数&lt;/h3&gt;
&lt;p&gt;这个很好理解，因为一般来说，空格子越少对玩家越不利。所以我们认为空格越多的格局越好。&lt;/p&gt;
&lt;h3 id=&quot;孤立空格数&quot;&gt;孤立空格数&lt;/h3&gt;
&lt;p&gt;这个指标评价空格被分开的程度，空格越分散则格局越差。&lt;/p&gt;
&lt;p&gt;具体来说，2048-AI在评价格局时，对这些启发指标采用了加权策略。具体代码如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-js&quot;&gt;// static evaluation function
AI.prototype.eval = function() {
    var emptyCells = this.grid.availableCells().length;

    var smoothWeight = 0.1,
        //monoWeight   = 0.0,
        //islandWeight = 0.0,
        mono2Weight  = 1.0,
        emptyWeight  = 2.7,
        maxWeight    = 1.0;

    return this.grid.smoothness() * smoothWeight
        //+ this.grid.monotonicity() * monoWeight
        //- this.grid.islands() * islandWeight
        + this.grid.monotonicity2() * mono2Weight
        + Math.log(emptyCells) * emptyWeight
        + this.grid.maxValue() * maxWeight;
};
&lt;/pre&gt;
&lt;p&gt;有兴趣的同学可以调整一下权重看看有什么效果。&lt;/p&gt;
&lt;h2 id=&quot;对对方选择的剪枝&quot;&gt;对对方选择的剪枝&lt;/h2&gt;
&lt;p&gt;在这个程序中，除了采用Alpha-beta剪枝外，在min节点还采用了另一种剪枝，即只考虑对方走出让格局最差的那一步（而实际2048中计算机的选择是随机的），而不是搜索全部对方可能的走法。这是因为对方所有可能的选择为“空格数×2”，如果全部搜索的话会严重限制搜索深度。&lt;/p&gt;
&lt;p&gt;相关剪枝代码如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-js&quot;&gt;// try a 2 and 4 in each cell and measure how annoying it is
// with metrics from eval
var candidates = [];
var cells = this.grid.availableCells();
var scores = { 2: [], 4: [] };
for (var value in scores) {
    for (var i in cells) {
        scores[value].push(null);
        var cell = cells[i];
        var tile = new Tile(cell, parseInt(value, 10));
        this.grid.insertTile(tile);
        scores[value][i] = -this.grid.smoothness() + this.grid.islands();
        this.grid.removeTile(cell);
    }
}

// now just pick out the most annoying moves
var maxScore = Math.max(Math.max.apply(null, scores[2]), Math.max.apply(null, scores[4]));
for (var value in scores) { // 2 and 4
    for (var i=0; i&lt;scores[value].length; i++) {
        if (scores[value][i] == maxScore) {
            candidates.push( { position: cells[i], value: parseInt(value, 10) } );
        }
    }
}
&lt;/pre&gt;
&lt;h2 id=&quot;搜索深度&quot;&gt;搜索深度&lt;/h2&gt;
&lt;p&gt;在2048-AI的实现中，并没有限制搜索的最大深度，而是限制每次“思考”的时间。这里设定了一个超时时间，默认为100ms，在这个时间内，会从1开始，搜索到所能达到的深度。相关代码：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-js&quot;&gt;// performs iterative deepening over the alpha-beta search
AI.prototype.iterativeDeep = function() {
    var start = (new Date()).getTime();
    var depth = 0;
    var best;
    do {
        var newBest = this.search(depth, -10000, 10000, 0 ,0);
        if (newBest.move == -1) {
            //console.log(&#39;BREAKING EARLY&#39;);
            break;
        } else {
            best = newBest;
        }
        depth++;
    } while ( (new Date()).getTime() - start &lt; minSearchTime);
    //console.log(&#39;depth&#39;, --depth);
    //console.log(this.translate(best.move));
    //console.log(best);
    return best
}
&lt;/pre&gt;
&lt;p&gt;因此这个算法实现的效果实际上依赖于执行javascript引擎机器的性能。当然可以通过增加超时时间来达到更好的效果，但此时每一步行走速度会相应变慢。&lt;/p&gt;
&lt;h2 id=&quot;算法的改进&quot;&gt;算法的改进&lt;/h2&gt;
&lt;p&gt;目前这个实现作者声称成功合成2048的概率超过90%，但是合成4096甚至8192的概率并不高。作者在&lt;a href=&quot;https://github.com/ov3y/2048-AI/blob/master/README.md&quot;&gt;github项目的REAMDE&lt;/a&gt;中同时给出了一些优化建议，这些建议包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存结果。目前这个实现并没有对已搜索的树做缓存，每一步都要重新开始搜索。&lt;/li&gt;
&lt;li&gt;多线程搜索。由于javascript引擎的单线程特性，这一点很难做到，但如果在其它平台上也许也可考虑并行技术。&lt;/li&gt;
&lt;li&gt;更好的启发函数。也许可以总结出一些更好的启发函数来评价格局价值。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;参考文献&quot;&gt;参考文献&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://gabrielecirulli.github.io/2048/&quot;&gt;2048 Game&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ov3y/2048-AI&quot;&gt;2048-AI github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.flyingmachinestudios.com/programming/minimax/&quot;&gt;An Exhaustive Explanation of Minimax, a Staple AI Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.neverstopbuilding.com/minimax&quot;&gt;Tic Tac Toe: Understanding the Minimax Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://cs.ucla.edu/~rosen/161/notes/alphabeta.html&quot;&gt;CS 161 Recitation Notes - Minimax with Alpha Beta Pruning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description> 
        </item> 
        
        <item> 
            <title>抓取网页内容生成Kindle电子书</title> 
            <link>http://blog.codinglabs.org/articles/convert-html-to-kindle-book.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/convert-html-to-kindle-book.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Thu, 27 Mar 2014 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;自从买了kindle后，总是想着如何最大效用发挥其效用。虽然多看上有很多书可以购买，网上也有很多免费的电子书，但是仍然有很多感兴趣的内容是以网页的形式存在的。例如&lt;a href=&quot;http://chimera.labs.oreilly.com/books/&quot;&gt;O’Reilly Atlas&lt;/a&gt;就提供了诸多电子书，但是只提供免费的在线阅读；另外还有很多资料或文档都只有网页形式。于是就希望通过某种方法讲这些在线资料转为epub或mobi格式，以便在kindle上阅读。这篇文章介绍了如何借助calibre并编写少量代码来达到这个目的。&lt;/p&gt;
&lt;h1 id=&quot;calibre&quot;&gt;Calibre&lt;/h1&gt;
&lt;h2 id=&quot;calibre简介&quot;&gt;Calibre简介&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://calibre-ebook.com/&quot;&gt;Calibre&lt;/a&gt;是一个免费的电子书管理工具，可以兼容Windows, OS X及Linux，令人欣喜的是，除了GUI外，calibre还提供了诸多命令行工具，其中的ebook-convert命令可以根据用户编写的recipes文件（实际是python代码）抓取指定页面内容并生成mobi等格式的电子书。通过编写recipes可以自定制抓取行为，以适应不同的网页结构。&lt;/p&gt;
&lt;h2 id=&quot;安装calibre&quot;&gt;安装Calibre&lt;/h2&gt;
&lt;p&gt;Calibre的下载地址是&lt;a href=&quot;http://calibre-ebook.com/download&quot;&gt;http://calibre-ebook.com/download&lt;/a&gt;，可以根据自己的操作系统下载相应的安装程序。&lt;/p&gt;
&lt;p&gt;如果是Linux操作系统，还可以通过软件仓库安装：&lt;/p&gt;
&lt;p&gt;Archlinux：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;pacman -S calibre
&lt;/pre&gt;
&lt;p&gt;Debian/Ubuntu：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;apt-get install calibre
&lt;/pre&gt;
&lt;p&gt;RedHat/Fedora/CentOS：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;yum -y install calibre
&lt;/pre&gt;
&lt;p&gt;注意，如果你使用OSX，需要单独安装&lt;a href=&quot;http://manual.calibre-ebook.com/cli/cli-index.html&quot;&gt;Command Line Tool&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id=&quot;抓取网页生成电子书&quot;&gt;抓取网页生成电子书&lt;/h1&gt;
&lt;p&gt;下面以&lt;a href=&quot;http://chimera.labs.oreilly.com/books/1230000000561&quot;&gt;Git Pocket Guide&lt;/a&gt;为例，说明如何通过calibre从网页生成电子书。&lt;/p&gt;
&lt;h2 id=&quot;找到index页&quot;&gt;找到index页&lt;/h2&gt;
&lt;p&gt;要抓取整本书，第一件事就是找到index页，这个页面一般是Table of Contents，也就是目录页，其中每个目录项链接到相应内容页。index页将会指导抓取哪些页面以及生成电子书时内容组织顺序。在这个例子中，index页面是&lt;a href=&quot;http://chimera.labs.oreilly.com/books/1230000000561/index.html&quot;&gt;http://chimera.labs.oreilly.com/books/1230000000561/index.html&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;编写recipes&quot;&gt;编写recipes&lt;/h2&gt;
&lt;p&gt;Recipes是一个以recipe为扩展名的脚本，内容实际上是一段python代码，用来定义calibre抓取页面的范围和行为，下面是用于抓取Git Pocket Guide的recipes：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-python&quot;&gt;from calibre.web.feeds.recipes import BasicNewsRecipe

class Git_Pocket_Guide(BasicNewsRecipe):

    title = &#39;Git Pocket Guide&#39;
    description = &#39;&#39;
    cover_url = &#39;http://akamaicovers.oreilly.com/images/0636920024972/lrg.jpg&#39;

    url_prefix = &#39;http://chimera.labs.oreilly.com/books/1230000000561/&#39;
    no_stylesheets = True
    keep_only_tags = [{ &#39;class&#39;: &#39;chapter&#39; }]

    def get_title(self, link):
        return link.contents[0].strip()

    def parse_index(self):
        soup = self.index_to_soup(self.url_prefix + &#39;index.html&#39;)

        div = soup.find(&#39;div&#39;, { &#39;class&#39;: &#39;toc&#39; })

        articles = []
        for link in div.findAll(&#39;a&#39;):
            if &#39;#&#39; in link[&#39;href&#39;]:
                continue

            if not &#39;ch&#39; in link[&#39;href&#39;]:
                continue

            til = self.get_title(link)
            url = self.url_prefix + link[&#39;href&#39;]
            a = { &#39;title&#39;: til, &#39;url&#39;: url }

            articles.append(a)

        ans = [(&#39;Git_Pocket_Guide&#39;, articles)]

        return ans
&lt;/pre&gt;
&lt;p&gt;下面分别解释代码中不同部分。&lt;/p&gt;
&lt;h3 id=&quot;总体结构&quot;&gt;总体结构&lt;/h3&gt;
&lt;p&gt;总体来看，一个recipes就是一个python class，只不过这个class必须继承calibre.web.feeds.recipes.BasicNewsRecipe。&lt;/p&gt;
&lt;h3 id=&quot;parse_index&quot;&gt;parse_index&lt;/h3&gt;
&lt;p&gt;整个recipes的核心方法是parse_index，也是recipes唯一必须实现的方法。这个方法的目标是通过分析index页面的内容，返回一个稍显复杂的数据结构（稍后介绍），这个数据结构定义了整个电子书的内容及内容组织顺序。&lt;/p&gt;
&lt;h3 id=&quot;总体属性设置&quot;&gt;总体属性设置&lt;/h3&gt;
&lt;p&gt;在class的开始，定义了一些全局属性：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-python&quot;&gt;title = &#39;Git Pocket Guide&#39;
description = &#39;&#39;
cover_url = &#39;http://akamaicovers.oreilly.com/images/0636920024972/lrg.jpg&#39;

url_prefix = &#39;http://chimera.labs.oreilly.com/books/1230000000561/&#39;
no_stylesheets = True
keep_only_tags = [{ &#39;class&#39;: &#39;chapter&#39; }]
&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;title：电子书标题&lt;/li&gt;
&lt;li&gt;description：电子书描述&lt;/li&gt;
&lt;li&gt;cover_url：电子书的封面图片&lt;/li&gt;
&lt;li&gt;url_prefix：这是我自用的属性，是内容页面的前缀，用于后面拼装内容页的完整url&lt;/li&gt;
&lt;li&gt;no_stylesheets：不要使用页面CSS样式&lt;/li&gt;
&lt;li&gt;keep_only_tags：这一行告诉calibre分析index页时仅考虑class属性为“chapter”的DOM元素，如果你看index页的源码会发现这对应一级标题。之所以这样是因为在这个例子中，index页面每个一级标题对应一个独立内容页，而二级标题仅链接到页面中某个锚点（anchor），所以仅需考虑一级标题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;parse_index返回值&quot;&gt;parse_index返回值&lt;/h3&gt;
&lt;p&gt;下面介绍parse_index需要通过分析index页面返回的数据结构。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/convert-html-to-kindle-book/01.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;总体返回数据结构是一个list，其中每个元素是一个tuple，一个tuple表示一卷（volume）。在这个例子中只有一卷，所以list中只有一个tuple。&lt;/p&gt;
&lt;p&gt;每个tuple有两个元素，第一个元素是卷名，第二个元素是一个list，list中每个元素是一个map，表示一章（chapter），map中有两个元素：title和url，title是章节标题，url是章节所在内容页的url。&lt;/p&gt;
&lt;p&gt;Calibre会根据parse_index的返回结果抓取并组织整个书，并且会自行抓取并处理内容中外链的图片。&lt;/p&gt;
&lt;p&gt;整个parse_index使用soup解析index页并生成上述数据结构。&lt;/p&gt;
&lt;h3 id=&quot;更多&quot;&gt;更多&lt;/h3&gt;
&lt;p&gt;上面是最基本的recipes，想了解更多的使用方法，可以参考&lt;a href=&quot;http://manual.calibre-ebook.com/news_recipe.html&quot;&gt;API文档&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;生成mobi&quot;&gt;生成mobi&lt;/h2&gt;
&lt;p&gt;编写好recipes后，在命令行下通过如下命令即可生成电子书：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;ebook-convert Git_Pocket_Guide.recipe Git_Pocket_Guide.mobi
&lt;/pre&gt;
&lt;p&gt;即可生成mobi格式的电子书。ebook-convert会根据recipes代码自行抓取相关内容并组织结构。&lt;/p&gt;
&lt;h1 id=&quot;最终效果&quot;&gt;最终效果&lt;/h1&gt;
&lt;p&gt;下面是在kindle上看到的效果。&lt;/p&gt;
&lt;p&gt;目录&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/convert-html-to-kindle-book/02.jpg&quot;/&gt;&lt;/p&gt;

&lt;p&gt;内容一&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/convert-html-to-kindle-book/03.jpg&quot;/&gt;&lt;/p&gt;

&lt;p&gt;内容二&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/convert-html-to-kindle-book/04.jpg&quot;/&gt;&lt;/p&gt;

&lt;p&gt;含有图片的页&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/convert-html-to-kindle-book/05.jpg&quot;/&gt;&lt;/p&gt;

&lt;p&gt;实际效果&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/convert-html-to-kindle-book/06.jpg&quot;/&gt;&lt;/p&gt;

&lt;h1 id=&quot;我的recipes仓库&quot;&gt;我的recipes仓库&lt;/h1&gt;
&lt;p&gt;我在github上建了一个&lt;a href=&quot;https://github.com/ericzhang-cn/kindle-open-books/&quot;&gt;kindle-open-books&lt;/a&gt;，里面放了一些recipes，有我写的，也有其他同学贡献的。欢迎任何人贡献的recipes。&lt;/p&gt;
</description> 
        </item> 
        
        <item> 
            <title>开始使用Ubuntu作为工作环境</title> 
            <link>http://blog.codinglabs.org/articles/getting-started-with-ubuntu.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/getting-started-with-ubuntu.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Mon, 30 Dec 2013 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;2012年3月，我自购了一台13寸的Macbook Air，从那时开始至今近两年时间，我一直用它作为工作本。但是最近越来越觉得4G的内存和128G的SSD力不从心，苦于Air无法升级硬件，于是终于下决心拿出入职时公司给配的Dell E6410，自己买了内存和SSD，升级成了8G内存+370G混合硬盘（120G SSD做主盘，250G硬盘做从盘）。&lt;/p&gt;
&lt;p&gt;硬件升级事小，关键是系统的迁移代价比较大。我在Dell本上装的是Ubuntu 13.10，由于我平时习惯使用Dropbox等云端服务，浏览器配置也都通过Google账号漫游，所以这部分迁移几乎没有成本，主要的成本在开发环境配置和常用软件迁移。虽然都是Unix系，但是Mac OSX下很多软件Linux下并没有。&lt;/p&gt;
&lt;p&gt;花了大约一个周末，总算把我的Ubuntu配置的比较顺手了，目前也已经正式投入工作。其中的重头戏便是开发环境（主要是terminal和vim）的配置，另外就是一些常用工具。这篇文章记录了我一些主要的工作，算是给自己留一个文档，也希望能给打算从Mac迁移到Linux的同学做一个借鉴。&lt;/p&gt;
&lt;h1 id=&quot;开发环境配置&quot;&gt;开发环境配置&lt;/h1&gt;
&lt;p&gt;之前在Mac下，我直接使用的是&lt;a href=&quot;https://github.com/square/maximum-awesome&quot;&gt;maximum-awesome&lt;/a&gt;，开发环境这块完全不用自己操心。可惜maximum-awesome只能在Mac下使用，并没有Linux版。于是需要自己做一些工作，以便让开发环境够顺手。&lt;/p&gt;
&lt;h2 id=&quot;使用terminator作为终端&quot;&gt;使用terminator作为终端&lt;/h2&gt;
&lt;h3 id=&quot;安装terminator&quot;&gt;安装terminator&lt;/h3&gt;
&lt;p&gt;Ubuntu自带的终端是gnome-terminal，虽然也还不错，但是不能支持屏幕分割、选择复制等功能让我很不爽，于是我换用terminator作为终端，terminator可以支持屏幕分割，并且默认快捷键和gnome-terminal无异，熟悉gnome-terminal的话可以快速上手。&lt;/p&gt;
&lt;p&gt;Ubuntu下可以这样安装terminator：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo apt-get install terminator
&lt;/pre&gt;
&lt;h3 id=&quot;terminator常用快捷键&quot;&gt;terminator常用快捷键&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ctrl-Shift-c 拷贝&lt;/li&gt;
&lt;li&gt;Ctrl-Shift-v 粘贴&lt;/li&gt;
&lt;li&gt;Ctrl-Shift-t 开新Tab窗口&lt;/li&gt;
&lt;li&gt;Ctrl-Shift-o 上下拆分屏幕&lt;/li&gt;
&lt;li&gt;Ctrl-Shift-e 左右拆分屏幕&lt;/li&gt;
&lt;li&gt;Ctrl-Shift-w 关闭当前窗口&lt;/li&gt;
&lt;li&gt;Ctrl-Shift-q 关闭整个终端&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;配置terminator使用solarized配色&quot;&gt;配置terminator使用solarized配色&lt;/h2&gt;
&lt;h3 id=&quot;使用terminator-solarized&quot;&gt;使用terminator-solarized&lt;/h3&gt;
&lt;p&gt;maximum-awesome所使用的&lt;a href=&quot;http://ethanschoonover.com/solarized&quot;&gt;solarized&lt;/a&gt;配色是相当不错的，所以自然希望继续使用。针对terminator的solarized配色已经有人专门做好了：&lt;a href=&quot;https://github.com/ghuntley/terminator-solarized&quot;&gt;terminator-solarized&lt;/a&gt;，只要按如下操作就可以使用：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;mkdir -p ~/.config/terminator/
curl https://raw.github.com/ghuntley/terminator-solarized/master/config &gt; ~/.config/terminator/config
&lt;/pre&gt;
&lt;p&gt;然后重新打开terminator就已经是solarized配色了。&lt;/p&gt;
&lt;h3 id=&quot;对terminator更多的配置&quot;&gt;对terminator更多的配置&lt;/h3&gt;
&lt;p&gt;接下来，可以在terminator-solarized配置文件的基础上进行更多的配置，例如背景透明、启用选择复制等。&lt;/p&gt;
&lt;p&gt;关于terminator的详细配置选项可以参考&lt;a href=&quot;http://manpages.ubuntu.com/manpages/intrepid/man5/terminator_config.5.html&quot;&gt;terminator manpage&lt;/a&gt;，下面贴出我的~/.config/terminator/config供参考：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;[global_config]
    title_transmit_bg_color = &quot;#d30102&quot;
    focus = system
    suppress_multiple_term_dialog = True
[keybindings]
[profiles]
    [[default]]
        palette = &quot;#073642:#dc322f:#859900:#b58900:#268bd2:#d33682:#2aa198:#eee8d5:#002b36:#cb4b16:#586e75:#657b83:#839496:#6c71c4:#93a1a1:#fdf6e3&quot;
        copy_on_selection = True
        background_image = None
        background_darkness = 0.95
        background_type = transparent
        use_system_font = False
        cursor_color = &quot;#eee8d5&quot;
        foreground_color = &quot;#839496&quot;
        show_titlebar = False
        font = Monospace 11
        background_color = &quot;#002b36&quot;
    [[solarized-dark]]
        palette = &quot;#073642:#dc322f:#859900:#b58900:#268bd2:#d33682:#2aa198:#eee8d5:#002b36:#cb4b16:#586e75:#657b83:#839496:#6c71c4:#93a1a1:#fdf6e3&quot;
        background_color = &quot;#002b36&quot;
        background_image = None
        cursor_color = &quot;#eee8d5&quot;
        foreground_color = &quot;#839496&quot;
    [[solarized-light]]
        palette = &quot;#073642:#dc322f:#859900:#b58900:#268bd2:#d33682:#2aa198:#eee8d5:#002b36:#cb4b16:#586e75:#657b83:#839496:#6c71c4:#93a1a1:#fdf6e3&quot;
        background_color = &quot;#fdf6e3&quot;
        background_image = None
        cursor_color = &quot;#002b36&quot;
        foreground_color = &quot;#657b83&quot;
[layouts]
    [[default]]
        [[[child1]]]
            type = Terminal
            parent = window0
            profile = default
        [[[window0]]]
            type = Window
            parent = &quot;&quot;
[plugins]
&lt;/pre&gt;
&lt;h3 id=&quot;配置dircolors&quot;&gt;配置dircolors&lt;/h3&gt;
&lt;p&gt;完成上述配置后，你会发现用ls命令查看目录和文件时是一片灰色。这是因为默认情况下solarized各种bright方案基本都是灰色，而系统默认显示目录和文件时多用bright色，此时需要配置dircolors才能显示出彩色的文件和目录。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/seebi/dircolors-solarized&quot;&gt;dircolors-solarized&lt;/a&gt;项目提供了适合于solarized的dircolors配色方案，只要选择合适的方案使用就可以了。例如我是用的solarized dark配色，所以可以选择适合这个配色的dircolors.ansi-dark：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;curl https://raw.github.com/seebi/dircolors-solarized/master/dircolors.ansi-dark &gt; ~/.dircolors
&lt;/pre&gt;
&lt;p&gt;然后在~/.bashrc中加入如下配置：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;# enable color support of ls and also add handy aliases
if [ -x /usr/bin/dircolors ]; then
    test -r ~/.dircolors &amp;&amp; eval &quot;$(dircolors -b ~/.dircolors)&quot; || eval &quot;$(dircolors -b)&quot;
    alias ls=&#39;ls --color=auto&#39;
    #alias dir=&#39;dir --color=auto&#39;
    #alias vdir=&#39;vdir --color=auto&#39;

    alias grep=&#39;grep --color=auto&#39;
    alias fgrep=&#39;fgrep --color=auto&#39;
    alias egrep=&#39;egrep --color=auto&#39;
fi

# some more ls aliases
alias ll=&#39;ls -alF&#39;
alias la=&#39;ls -A&#39;
alias l=&#39;ls -CF&#39;
&lt;/pre&gt;
&lt;p&gt;执行&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;source ~/.bashrc
&lt;/pre&gt;
&lt;p&gt;后，再执行ls或ll就可以看到彩色的目录或文件了。&lt;/p&gt;
&lt;p&gt;配置完的terminator效果如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/getting-started-with-ubuntu/01.png&quot;/&gt;&lt;/p&gt;

&lt;h2 id=&quot;配置vim&quot;&gt;配置VIM&lt;/h2&gt;
&lt;p&gt;vim作为我日常使用频率最高的代码编辑器，自然要好好配置一番。之前的maximum-awesome自带了很多vim插件，这里我没有精力完全按照maximum-awesome的插件配置，只选取了日常比较常用的几个插件先配上，后面需要的话可以再加。&lt;/p&gt;
&lt;h3 id=&quot;插件&quot;&gt;插件&lt;/h3&gt;
&lt;p&gt;我目前安装的插件有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/scrooloose/nerdtree&quot;&gt;NERDTree&lt;/a&gt;：可以在单独的window中浏览目录和文件，方便打开的选取文件。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/vim-scripts/taglist.vim&quot;&gt;taglist&lt;/a&gt;：可以通过ctags生成的tag文件索引定位代码中的常量、函数、类等结构，阅读代码和写代码必备。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/Lokaltog/vim-powerline&quot;&gt;powerline&lt;/a&gt;：在底部显示一个非常漂亮的状态条，还可以通过不同的颜色提醒用户当前处于什么状态（如normal、insert或visual）。&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/altercation/vim-colors-solarized&quot;&gt;vim-colors-solarized&lt;/a&gt;：vim的solarized配色插件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果所有插件都按vim标准方法安装，各种插件会非常分散，不便于管理，于是我选用&lt;a href=&quot;https://github.com/tpope/vim-pathogen&quot;&gt;pathogen&lt;/a&gt;安装和管理vim插件。pathogen允许将各个插件放在.vim/bundle/下各自的目录中，通过启动时自动加载所有插件。&lt;/p&gt;
&lt;h3 id=&quot;自动配置工具&quot;&gt;自动配置工具&lt;/h3&gt;
&lt;p&gt;整个配置过程过于繁琐不再赘述，我已经将配置过程写成了一个自动配置脚本并放到了github：&lt;a href=&quot;https://github.com/ericzhang-cn/vim-conf&quot;&gt;https://github.com/ericzhang-cn/vim-conf&lt;/a&gt;，需要的朋友只要clone下来并运行init.sh脚本就可以自动完成整个配置：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;git clone https://github.com/ericzhang-cn/vim-conf.git
cd vim-conf &amp;&amp; ./init.sh
&lt;/pre&gt;
&lt;p&gt;最终配置效果如下：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/getting-started-with-ubuntu/02.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;（更新：当前我已经换用&lt;a href=&quot;https://github.com/ericzhang-cn/maximum-awesome-linux&quot;&gt;maximum-awesome-linux&lt;/a&gt;，不再维护之前那个配置脚本）&lt;/p&gt;
&lt;h3 id=&quot;快捷键&quot;&gt;快捷键&lt;/h3&gt;
&lt;p&gt;其中并没有对vim默认的快捷键做过多重设，只有两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;,-d：打开或关闭NERDTree&lt;/li&gt;
&lt;li&gt;,-t：打开或关闭taglist&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;（更新：换用maximum-awesome-linux后快捷键会不一样，具体请参考&lt;a href=&quot;https://github.com/ericzhang-cn/maximum-awesome-linux/blob/master/README.md&quot;&gt;README&lt;/a&gt;）&lt;/p&gt;
&lt;h1 id=&quot;常用工具&quot;&gt;常用工具&lt;/h1&gt;
&lt;h2 id=&quot;浏览器&quot;&gt;浏览器&lt;/h2&gt;
&lt;p&gt;Ubuntu自带的是Firefox，我平常使用的是Chromium，这点在Ubuntu下没任何问题，可以直接安装：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo apt-get install chromium-browser
&lt;/pre&gt;
&lt;p&gt;用Google账号登录后，书签、插件等会自动同步，非常方便。&lt;/p&gt;
&lt;h2 id=&quot;搜狗输入法-amp-谷歌输入法&quot;&gt;搜狗输入法&amp;谷歌输入法&lt;/h2&gt;
&lt;p&gt;Mac下有搜狗输入法或百度输入法。不过目前搜狗也基于fcitx做了linux版的搜狗输入法。&lt;/p&gt;
&lt;p&gt;Ubuntu自带的ibus直接卸掉，然后安装&lt;a href=&quot;https://fcitx-im.org/wiki/Fcitx&quot;&gt;fcitx&lt;/a&gt;：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo add-apt-repository ppa:fcitx-team/nightly &amp;&amp; sudo apt-get update
sudo apt-get install fcitx-sogoupinyin
&lt;/pre&gt;
&lt;p&gt;当然谷歌拼音也不错：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo apt-get install fcitx-googlepinyin
&lt;/pre&gt;
&lt;h2 id=&quot;邮件&quot;&gt;邮件&lt;/h2&gt;
&lt;p&gt;Mac下是使用Foxmail for Mac，linux下可以选择ThunderBird，用起来很顺手。&lt;/p&gt;
&lt;h2 id=&quot;dropbox&quot;&gt;Dropbox&lt;/h2&gt;
&lt;p&gt;Dropbox有官方linux客户端，可以到其官网下载安装。不过安装后也许会发现dropbox的icon没有出现在上面的indicator上，可以这样修复：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo apt-get install libappindicator1
dropbox stop &amp;&amp; dropbox start
&lt;/pre&gt;
&lt;h2 id=&quot;evernote&quot;&gt;Evernote&lt;/h2&gt;
&lt;p&gt;很不幸，我平常用来做笔记的evernote没有linux官方客户端，不过可以选择使用web版，或者安装第三方linux客户端everpad：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo add-apt-repository ppa:nvbn-rm/ppa
sudo apt-get update
sudo apt-get install everpad
&lt;/pre&gt;
&lt;p&gt;当然功能和美观程度都没法和Mac下官方的客户端相比，不过日常使用还是足够了。&lt;/p&gt;
&lt;h2 id=&quot;办公office&quot;&gt;办公Office&lt;/h2&gt;
&lt;p&gt;Ubuntu自带的LibreOffice可以很好的满足需求，试用了WPS for linux，无法正常打开Office 2010的文件，故而弃用之。LibreOffice打开Office 2010的文件没有问题。&lt;/p&gt;
&lt;h2 id=&quot;pdf及论文管理&quot;&gt;PDF及论文管理&lt;/h2&gt;
&lt;p&gt;平常使用的&lt;a href=&quot;http://www.mendeley.com/&quot;&gt;Mendeley&lt;/a&gt;有官方linux客户端，可以到官网下载。&lt;/p&gt;
&lt;h2 id=&quot;绘图及图像处理&quot;&gt;绘图及图像处理&lt;/h2&gt;
&lt;p&gt;平常工程文档做图一般用&lt;a href=&quot;http://www.yworks.com/en/products_yed_about.html&quot;&gt;yEd&lt;/a&gt;，是java开发的，所以在linux下可以直接使用。&lt;/p&gt;
&lt;p&gt;图像处理可以用gimp：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo apt-get install gimp
&lt;/pre&gt;
&lt;h2 id=&quot;影音播放&quot;&gt;影音播放&lt;/h2&gt;
&lt;p&gt;平常很少离线看视频，如果需要的话，vlc应该够了：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo apt-get install vlc
&lt;/pre&gt;
&lt;h2 id=&quot;截图工具&quot;&gt;截图工具&lt;/h2&gt;
&lt;p&gt;截图工具非shutter莫属：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;sudo apt-get install shutter
&lt;/pre&gt;
&lt;h2 id=&quot;阿里旺旺&quot;&gt;阿里旺旺&lt;/h2&gt;
&lt;p&gt;因为工作关系，平常必须使用旺旺交流。Mac下有非常好用的官方版，linux下并没有官方旺旺，有个内部版本巨烂无比。不过之前参加活动获赠一个&lt;a href=&quot;http://www.codeweavers.com/products/&quot;&gt;CrossOver&lt;/a&gt;正版授权序列号。&lt;/p&gt;
&lt;p&gt;目前CrossOver运行阿里旺旺2013非常流畅。&lt;/p&gt;
&lt;h2 id=&quot;qq&quot;&gt;QQ&lt;/h2&gt;
&lt;p&gt;这个对我来说不是刚需。CrossOver可以运行TM2013，另外WebQQ或开VirtualBox在虚拟机中运行QQ都可以。&lt;/p&gt;
&lt;h2 id=&quot;专业软件&quot;&gt;专业软件&lt;/h2&gt;
&lt;p&gt;其它常用软件特别是如R或Octave等专业软件，本身就有linux版，所以可以按需安装。如Git等开发工具本来就是linux下的软件，当然更不在话下。&lt;/p&gt;
&lt;p&gt;目前已经用Ubuntu工作了一段时间，总体来说从Mac转过来肯定需要一点适应，不过目前感觉没有遇到特别大的问题，毕竟同属Unix系，对码农来说大多数使用习惯几乎是无缝切换。&lt;/p&gt;
&lt;p&gt;另外，重新回到OSS的感觉不错！最后上张图吧：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/getting-started-with-ubuntu/03.png&quot;/&gt;&lt;/p&gt;
</description> 
        </item> 
        
        <item> 
            <title>一个故事告诉你比特币的原理及运作机制</title> 
            <link>http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Mon, 16 Dec 2013 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;周末花时间看了一些比特币原理相关的资料，虽然不敢说把每个细节都完全搞懂了，不过整体思路和关键部分的主要原理还是比较明白。写一篇文章分享给大家。这篇文章的定位会比较科普，尽量用类比的方法将比特币的基本原理讲出来。这篇文章不会涉及算法和协议中比较细节的部分，打算后面会再写一篇程序员视角下的比特币原理，那里会从技术人员的视角对比特币系统中较为关键的数据结构、算法和协议进行一些讲解。&lt;/p&gt;
&lt;p&gt;在这篇文章中我会给出一个虚拟的村庄叫“比特村”，整个文章会以讲故事的方式，逐步告诉大家比特币提出的动机、解决了什么问题以及一些关键组件的目标和设计方案。&lt;/p&gt;
&lt;h1 id=&quot;问题的提出&quot;&gt;问题的提出&lt;/h1&gt;
&lt;p&gt;我们先从比特币产生的动机开始。&lt;/p&gt;
&lt;h2 id=&quot;以物易物的比特村&quot;&gt;以物易物的比特村&lt;/h2&gt;
&lt;p&gt;话说在这个世界上，有一个叫比特村的小村庄，村庄共有几百户人家。这个村庄几乎与世隔绝，过着自给自足的生活。由于没有大规模贸易，比特村村民一直过着以物易物的生活，也就是说村民之间并没有使用统一的货币，互相间的贸易基本上就是老张家拿一袋面粉换老李家一只羊，王大嫂拿一筐野果换刘大婶两尺布。村民们一直就这么纯朴的生活着。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/01.png&quot;/&gt;&lt;/p&gt;

&lt;h2 id=&quot;实物货币&quot;&gt;实物货币&lt;/h2&gt;
&lt;p&gt;终于有一天，村民觉得一直这样以物易物实在太不方便了，于是村子全员开会，讨论如何解决这个问题。有人提议，以便于分割且稀有的东西，例如黄金，作为一般等价物，把其它物品和黄金的对应关系编成一张表格，例如一克黄金对应一只羊，一克黄金对应一袋面粉等等，此时老张再也不用扛着一袋面粉气喘吁吁的去老李家换羊了，他只要从家里摸出一克金子，就可以去老李家牵回一只羊，而老李拿着这一克黄金可以从任何愿意出让面粉的人那里换回一袋面粉，当然也可以换取任何和一克黄金等值的物品。&lt;/p&gt;
&lt;p&gt;此时比特村进入了实物货币时代。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/02.png&quot;/&gt;&lt;/p&gt;

&lt;h2 id=&quot;符号货币&quot;&gt;符号货币&lt;/h2&gt;
&lt;p&gt;好景不长，过了一段时间，实物货币的弊端也出现了。因为比特村附近金矿并不多，开采和冶炼金子太费时费力了。而随着使用，金子总是不断会因为磨损、丢失或有人故意囤积而发生损耗。全村人又一次坐在了一起，开始商讨对策。此时有人说，其实大家也不必一定要真的用黄金啊，随便找张纸，写上“一克黄金”，只要全村人都认同这张纸就等于一克黄金，问题不就解决了。其他人纷纷表示认同，但同时也有了新的问题：真实的黄金是需要开采和冶炼的，金矿有限，开采和冶炼也需要成本，所以没有人可以短期凭空制造大量的黄金，可写字就不同了，只要我纸够笔够，随便像写多少写多少，那这就变成拼谁家里纸多了，搞不好到时一万张纸才能换一只羊（实际上这就发生了经济学上的通货膨胀）。&lt;/p&gt;
&lt;p&gt;大家一想也是啊。不过此时又有人提出了解决方案：这个纸不是谁写都有效，我们只认村里德高望重的老村长写得，大家都认识老村长的字。老村长写一些纸，同时按照各家黄金存量发给大家等量的纸，例如老张家有二百克黄金，老村长就发给老张二百张写着“一克黄金”的纸，同时将老张家的黄金拿走作为抵押。就这样，老村长将村里所有黄金收归到自己的家里，并按各家上交的黄金数量发给等值的写有字的纸。此时村民就可以拿着这些纸当黄金进行贸易了，而且大家都认得老村长的字，其他人伪造不出来。另外，如果谁的纸磨损太严重，也可拿到老村长那里兑换新的等值的纸，另外老村长承诺任何人如果想要换成真黄金，只要拿纸回来，老村长就会把等值的黄金还给那人。因为老村长写得纸的黄金量和真实放在家里的黄金量是一样的，所以只要严格按照销毁多少纸新写多少纸的原则，每一张有效的纸总能换回相应的真黄金。&lt;/p&gt;
&lt;p&gt;此时，比特村进入了符号货币（纸币）时代。而老村长就承担了政府和银行的角色。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/03.png&quot;/&gt;&lt;/p&gt;

&lt;h2 id=&quot;中央系统虚拟货币&quot;&gt;中央系统虚拟货币&lt;/h2&gt;
&lt;p&gt;又过了几年，老村长由于每天都要核对大量的旧纸币，写新的纸币，还要把各种账目仔细做好记录。一来二去，老村长操劳过度不幸驾鹤西去了。&lt;/p&gt;
&lt;p&gt;比特村再次召开全体大会，讨论应该怎么办。此时老村长的儿子二狗子自告奋勇接过了父亲的笔，承担起货币发行的责任。这个年轻的村长二狗子很聪明，他做了几天，发现好像也不用真的写那么多纸。完全可以这样：村民把纸币都交上来，销毁，但是二狗子会记录下每户上交的纸币数量。以后如果要进行付钱，例如老张要拿一克金子向老李换一只羊，就一起给二狗子打个电话，说明要将老张名下的一克金子划归老李名下，二狗子拿出账本，看看老张名下是否有一克金子，如果有就在老张的名下减掉一克，在老李的名下加上一克，这样就完成了支付，此时老李在电话中听到二狗子确认转账完成，就可以放心让老张把羊牵走了。&lt;/p&gt;
&lt;p&gt;此时比特村进入了中央系统虚拟货币时代。每个村民都不需要用实物支付，支付过程变成了二狗子那边维护的账本上数字的变更。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/04.png&quot;/&gt;&lt;/p&gt;

&lt;h2 id=&quot;分布式虚拟货币&quot;&gt;分布式虚拟货币&lt;/h2&gt;
&lt;p&gt;这新上任的二狗子是聪明，不过这人有时候是聪明反被聪明误。有一天二狗子盯着这账本，心想这全村各户谁有多少钱就是我说的算，那我岂不是……。于是他头脑一热，私自从老张帐下划了十克金子到自己名下。&lt;/p&gt;
&lt;p&gt;本以为天衣无缝，但没想到老张也有记账的习惯，有一天他正要付钱却被二狗子告知账户没钱了。老张核对了一下自己的账本，明明还有十克啊，于是拿着账本去找二狗子理论，这一核对发现了那笔未经老张同意的转账。&lt;/p&gt;
&lt;p&gt;东窗事发！比特村炸开锅了。二狗子被弹劾是不可避免了，不过通过这件事，大家发现了账本集中在一个人手里的弊端：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这个体系完全依赖于账本持有人的个人信用，如果这个人不守规矩，随意篡改账本，那么整个货币系统就会崩溃&lt;/li&gt;
&lt;li&gt;如果这个人家里失火或者账本失窃，同样也会为整个体系带来毁灭性的打击&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;正当人们不知所措时，村里一个叫中本聪的宅男科学家走上了台，告诉大家他已经设计了一套不依赖任何中央处理人的叫比特币的虚拟货币系统，可以解决上述问题。然后他缓缓讲述了自己的方案。&lt;/p&gt;
&lt;p&gt;下面我们就来看看中本聪同学是如何设计这套系统的。&lt;/p&gt;
&lt;h1 id=&quot;基础设施搭建&quot;&gt;基础设施搭建&lt;/h1&gt;
&lt;h2 id=&quot;账簿公开机制&quot;&gt;账簿公开机制&lt;/h2&gt;
&lt;p&gt;中本聪首先说明，要对现有账簿进行如下改造：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;账簿上不再记载每户村民的余额，而只记载每一笔交易。即记载每一笔交易的付款人、收款人和付款金额。只要账簿的初始状态确定，每一笔交易记录可靠并有时序，当前每个人持有多少钱是可以推算出来的。&lt;/li&gt;
&lt;li&gt;账簿由私有改为公开，只要任何村民需要，都可以获得当前完整的账簿，账簿上记录了从账簿创建开始到当前所有的交易记录。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此言一出，下面立刻炸锅了。第一条还无所谓，但是第二条简直无法接受，因为账簿可是记录了所有村民的交易，这样大家的隐私不全暴露了吗。&lt;/p&gt;
&lt;p&gt;中本聪倒是不慌不忙，拿出了一对奇怪的东西。&lt;/p&gt;
&lt;h2 id=&quot;身份与签名机制（公钥加密系统）&quot;&gt;身份与签名机制（公钥加密系统）&lt;/h2&gt;
&lt;p&gt;中本聪说，大家不要慌。在他的这套机制下，任何人都不使用真实身份交易，而是使用一个唯一的代号交易。&lt;/p&gt;
&lt;p&gt;他展示了手里神奇的东西，说这两件东西分别叫保密印章和印章扫描器。后面他会给村里每一户发一个保密印章和一个印章扫描器。两者的作用如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保密印章可以在纸上盖一个章，每个印章盖出的章都隐含了一个全村唯一的一串字符，但是凭肉眼是看不出来的。也无法通过观察来制造出相应的印章。&lt;/li&gt;
&lt;li&gt;印章扫描器可以扫描某个已经盖好的章，读出隐含的信息，并在液晶屏上显示出一串字符。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了这两个神奇的东西，大家就可以在不暴露真实身份的情况下进行交易了，而印章隐含的那一串字符就是这户人家的代号。具体如何巧妙利用保密印章和印章扫描器进行交易，会在下文详述。&lt;/p&gt;
&lt;h2 id=&quot;成立虚拟矿工组织（挖矿群体）&quot;&gt;成立虚拟矿工组织（挖矿群体）&lt;/h2&gt;
&lt;p&gt;下一步，中本聪面向全村招募虚拟矿工，招募要求如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;矿工以组为单位，一组可以是单独的一户，也可以是几户联合为一组&lt;/li&gt;
&lt;li&gt;成为矿工不影响正常使用货币&lt;/li&gt;
&lt;li&gt;矿工每天要花费一定时间从事比特币“挖矿”活动，但是不同于挖金矿，虚拟矿工不需要拿着工具去野外作业，在家里就可以完成工作&lt;/li&gt;
&lt;li&gt;矿工有一定可能性获得报酬，在挖矿活动中付出的努力越多，获得报酬的可能性越大&lt;/li&gt;
&lt;li&gt;矿工可以随时退出，也可以随时有新的矿工加进来&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;很快，大约有五分之一的村民加入比特币矿工组织，共分成了7个组。&lt;/p&gt;
&lt;h2 id=&quot;建立初始账簿（创世块）&quot;&gt;建立初始账簿（创世块）&lt;/h2&gt;
&lt;p&gt;下面，中本聪宣布，先根据二狗子手里的账簿，把抵押的所有黄金按账簿记录的余额退还给每位村民，然后彻底销毁这本账簿。&lt;/p&gt;
&lt;p&gt;然后，中本聪拿出一本新账簿，在账簿的第一页上记录了一些交易记录，特别的是，这些记录的付款人一栏全都是“系统”，而收款人分别是每个印章对应的隐含字符，代表初始时刻，系统为每一户默认分配了一定数量比特币，但是数量非常少，都只有几枚，甚至有些不幸的村户没有获得比特币。&lt;/p&gt;
&lt;p&gt;接着中本聪说，由于目前市面上比特币非常少，大家可以先回到用黄金做货币的时代，由于我不是村长，我也没有权利强迫大家一定要承认比特币，大家可以自行决定要不要接受比特币。不过随着比特币的流动和矿工的活动，比特币会慢慢多起来。&lt;/p&gt;
&lt;h1 id=&quot;支付与交易&quot;&gt;支付与交易&lt;/h1&gt;
&lt;p&gt;做了这么多铺垫，终于说到重点了，下面说一下在这样一个体系下如何完成支付。以老张付给老李10个比特币为例。&lt;/p&gt;
&lt;h3 id=&quot;付款人签署交易单&quot;&gt;付款人签署交易单&lt;/h3&gt;
&lt;p&gt;为了支付10个比特币，老张首先要询问老李的标识字符串，例如是“ABCDEFG”，同时老张也有一个标识字符串例如是“HIJKLMN”，然后老张写一张单子，内容为“HILKLMN支付10比特币给ABCDEFG”，然后用自己的保密印章改一个章，将这张单子交给老李。另外为了便于追溯这笔钱的来源，还要在单子里注明这笔钱的来源记在哪一页，例如这个单子里，老张的10比特币来自建立账簿时系统的赠送，记录在账簿第一页。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/05.png&quot;/&gt;&lt;/p&gt;

&lt;h3 id=&quot;收款人确认单据签署人&quot;&gt;收款人确认单据签署人&lt;/h3&gt;
&lt;p&gt;老李拿到这个单子后，需要确认这个单子确实是来自“HIJKLMN”这个人（也就是老张）签署的，这个并不困难。因为单子上必须有保密章，老李拿出印章扫描器，扫一下章，如果液晶屏显示出的字符和付款人字符是一致的（这里是“HIJKLMN”），就可以确认单子确实是付款人签署的。这是因为根据保密印章的机制，没有其他人可以伪造印章，任何一个人只要扫描一下印章，都可以确认单子的付款人和盖章人是否一致。&lt;/p&gt;
&lt;h3 id=&quot;收款人确认付款人余额&quot;&gt;收款人确认付款人余额&lt;/h3&gt;
&lt;p&gt;这个系统到目前还是很有问题。通过保密印章，收款人虽然可以确认付款人确实签署了这份单子，但是无法自行确认付款人是否有足够的余额支付。之前的中央虚拟货币系统中，二狗子负责检查付款人的余额，并通知收款人交易是否有效，现在把二狗子开了，谁来负责记账和确认每笔交易的有效性呢？&lt;/p&gt;
&lt;p&gt;之前说过，中本聪设计的这个系统是分布式货币系统，不依赖任何中央人物，所以不会有一个或少数几个人负责这件事，最终承担这份工作的是之前所提到的矿工组织。老张、老李和全村其他任何使用比特币进行交易的村民都依赖矿工组织的工作才能完成交易。&lt;/p&gt;
&lt;h2 id=&quot;矿工的工作&quot;&gt;矿工的工作&lt;/h2&gt;
&lt;p&gt;矿工的工作是整个系统的核心，也是最复杂性最高的地方。下面逐步介绍矿工的工作内容和目的。&lt;/p&gt;
&lt;h3 id=&quot;矿工的工具&quot;&gt;矿工的工具&lt;/h3&gt;
&lt;p&gt;俗话说，工欲善其事，必先利其器。比特币矿工虽然不用铁撅、铁锨和探照灯等工具，不过也要有一些必备的东西。&lt;/p&gt;
&lt;p&gt;初始账簿。每个组首先自己复制一份初始账簿，初始账簿只有一页，记录了系统的第一次赠送&lt;/p&gt;
&lt;p&gt;空账簿纸。每个小组有若干账簿纸，每一页纸上仅有账簿结构，没有填内容，具体内容的书写规则后面讲述。下面是一张空账簿纸的样子，各个字段的意义后面会说到&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/06.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;编码生成器（哈希函数）。中本聪又向矿工组织的每个组分发了若干编码生成器，这个东西很神奇，将一页账簿填好内容的账簿纸放入这个机器，机器会在账簿纸的“本账单编号”一栏自动打印一串由“0”和“1”组成的编号，共256个。最神奇的是，编号生成器有如下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成的编号仅与账簿纸上填入的内容有关，与填写人、字体、填写时间等因素均无关&lt;/li&gt;
&lt;li&gt;内容相同的账簿纸生成的编号总是相同，但是如果内容哪怕只改一个字符，编号就会面目全非&lt;/li&gt;
&lt;li&gt;编码生成器在打印编码时还需要将所有填入账簿纸的交易单放入，机器会扫描交易单和填入交易单的一致性，尤其是保密印章，如果发现保密印章和付款人不一致，会拒绝打印编码&lt;/li&gt;
&lt;li&gt;将一张已打印的账簿纸放入，机器会判定编号是否是有效的机器打印，并且判定编号和内容是否一致，这个编号无法伪造&lt;/li&gt;
&lt;li&gt;交易单收件箱。每个矿工小组需要在门口挂一个箱子用于收集交易单。&lt;/li&gt;
&lt;li&gt;公告板。每个矿工小组同样需要一个公告板公示一些信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了上面的工具，矿工组织就可以开工了！&lt;/p&gt;
&lt;h3 id=&quot;收集交易单&quot;&gt;收集交易单&lt;/h3&gt;
&lt;p&gt;中本聪规定，每笔交易的发起人，不但要将交易单给到收款人，还要同时复制若干份一模一样的交易单投递到每个矿工小组的收件箱里。&lt;/p&gt;
&lt;p&gt;矿工小组的人定期到自己的收件箱里把收集到的交易单一并取出来。&lt;/p&gt;
&lt;h3 id=&quot;填写账簿&quot;&gt;填写账簿&lt;/h3&gt;
&lt;p&gt;此时小组的人拿出一张空的账簿纸，把这些交易填写到“交易清单”一栏，同时找到当前账簿最后一页，将最后一页的编号抄写到“上一张账单编号一栏”。
注意还有个“幸运数字”，可以随便填上一个数字，如12345。然后，将这样账簿纸放入编号生成器，打印好编号，一张账簿就算完成了。&lt;/p&gt;
&lt;p&gt;如果你以为矿工的工作就这么简单，那就大错特错了，中本聪有个变态的规定：只有编号的前10个数均为0，这页账簿纸才算有效。&lt;/p&gt;
&lt;p&gt;根据之前对编号生成器的描述，要修改编号，只能修改账簿纸的内容，而“交易清单”和“上一张账簿纸编号”是不能随便改的，那么只能改幸运数字了。于是为了生成有效的账簿纸，小组里的矿工就不断抄写账簿纸，但每张纸的幸运数字都不同，然后不断的重复将纸放入编码器，如果生成的编号不符合规定，这张纸就算废了，重复这个过程直到生成一串有效的编号。&lt;/p&gt;
&lt;p&gt;我们知道，如果编号的每一个数字都是随机的，那么平均写1000多张幸运数字不同的纸才能获得一个有效的编号。&lt;/p&gt;
&lt;p&gt;这就奇怪了，这些矿工为什么要拼命干这看似无意义的事情呢？还记得之前说过矿工有报酬吧，这就是矿工的动力了。中本聪规定：每一张账簿纸的交易清单第一条交易为“系统给这个小组支付50个比特币”。也就是说，如果你生成了一张有意义的账簿纸，并且被所有挖矿小组接受了，那么就意味着这条交易也被接受了，你的挖矿小组获得了50个比特币。&lt;/p&gt;
&lt;p&gt;这就是矿工被叫做矿工的原因，也是为什么之前说随着交易和矿工的活动，比特币的数量会不断增多。例如下面是一个挖矿过程，这个小组的公共比特币帐号为“UVWXYZ”。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/07.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;在幸运数字尝试到“533”时，系统生成了一页有效账簿。&lt;/p&gt;
&lt;h3 id=&quot;确认账簿&quot;&gt;确认账簿&lt;/h3&gt;
&lt;p&gt;当某挖矿小组幸运的生成了一张有意义的账簿，为了得到奖励，必须立刻请其它小组确认自己的工作。前面说过，当前村里有7个挖矿组，所以这个小组必须将有效账簿纸誊抄6份快马加鞭送到其他6个小组请求确认。&lt;/p&gt;
&lt;p&gt;中本聪规定，当某个小组接到其他小组送来的账簿纸时，必须立即停下手里的挖矿工作进行账簿确认。&lt;/p&gt;
&lt;p&gt;需要确认的信息有三个：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;账簿的编号有效&lt;/li&gt;
&lt;li&gt;账簿的前一页账簿有效&lt;/li&gt;
&lt;li&gt;交易清单有效&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;首先看第一个，这个确认比较简单。只要将送来的账簿纸放入编码生成器进行验证，如果验证通过，则编号有效。&lt;/p&gt;
&lt;p&gt;第二部分需要将账簿页上的“上一页账簿纸编号”和这个小组目前保存的有效账簿最后一页编号比对，如果相同则确认，如果不同，需要顺着已有账簿向前比对，直到找到这个编号的页。如果没有找到指定的“上一页账簿纸编号”对应的页，这个小组会将此页丢掉。不予确认。&lt;/p&gt;
&lt;p&gt;注意，由上面的机制可以保证，如果各个小组手里的账簿纸是相同的，那么他们都能按同样的顺序装订成相同的账簿。因为后面一张纸的编号总是依赖前面的纸的编号，编码生成器的机制保证了所有合法账簿纸的相对先后顺序在每个小组那里都是相同的（可能会有分支，但不会出现环，后面细讲）。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/08.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;最后是如何确认交易清单有效，其实也就是要确认当前每笔交易的付款人有足够的余额支付这笔钱。由于交易信息里包含这笔钱是如何来的，还包含了记录来源交易的账单编号。例如，HIJKLMN要给ABCDEFG10个比特币，并注明了这10个比特币来自之前OPQRST支付给HIJKLMN的一笔交易，确认时首先要确认之前这笔交易是否存在，同时还要检查HIJKLMN在这之前没有将这10个比特币支付给别人。这一切确认后，这笔交易有效性就被确认了。&lt;/p&gt;
&lt;p&gt;其中第一笔是系统奖励给生成这页账簿的小组的50个，这笔交易大家都默认承认，后面的只要按照上述方法追溯，就可以确认HIJKLMN是否当前真有10个比特币支付给ABCDEFG。&lt;/p&gt;
&lt;p&gt;如果完成了所有了上述验证并全部通过，这个小组就认可了上述账簿纸有效，然后将这张账簿纸并入小组的主账簿，舍弃目前正在进行的工作，后面的挖矿工作会基于这本更新后的主账本进行。&lt;/p&gt;
&lt;h3 id=&quot;账簿确认反馈&quot;&gt;账簿确认反馈&lt;/h3&gt;
&lt;p&gt;对于挖矿小组来说，当账簿纸送出去后，如果后面有收到其他小组送来的账簿纸，其“上一页账簿纸编号”为自己之前送出去的账簿纸，那么就表示他们的工作成功被其他小组认可了，因为已经有小组基于他们的账簿纸继续工作了。此时，可以粗略的说可以认为已经得到了50个比特币。&lt;/p&gt;
&lt;p&gt;另外，任何一个小组当新生成有效账簿纸或确认了别的小组的账簿纸时，就将最新被这个小组承认的交易写到公告牌上，那么收款人只要发现相关交易被各个小组认可了，基本就可以认为这笔钱已经到了自己的账上，后面他就可以在付款时将钱的来源指向这笔交易了。&lt;/p&gt;
&lt;p&gt;以上就是整个比特币的支付体系。下面我们来分析一下，这个体系为什么可以工作下去，以及这个体系可能面临的风险。&lt;/p&gt;
&lt;h1 id=&quot;工作机制分析&quot;&gt;工作机制分析&lt;/h1&gt;
&lt;p&gt;虽然上面阐述了比特币的基本运作规则，但是村民们还是有不少疑问。所以中本聪同学专门开了个答疑会，解答常见问题。下面总结一下村民最集中关心的问题。&lt;/p&gt;
&lt;h2 id=&quot;核心问题答疑&quot;&gt;核心问题答疑&lt;/h2&gt;
&lt;h3 id=&quot;如果同时收到两份合法的账簿页怎么办？&quot;&gt;如果同时收到两份合法的账簿页怎么办？&lt;/h3&gt;
&lt;p&gt;注意在上面的运行机制中，各个挖矿小组是并行工作的，因此完全可能出现这样的情况：某小组收到两份不一样的账簿页，它们都基于当前这个小组的主账簿的最后一页，并且内容也都完全合法，怎么办？&lt;/p&gt;
&lt;p&gt;关于这个问题，中本聪同学说，小组不应该以线性方式组织账簿，而应该以树状组织账簿，任何时刻，都以当前最长分支作为主账簿，但是保留其它分支。举个例子，某小组同时收到A、B两份账簿页，经核算都是合法的，此时小组应该将两页以分叉的形式组织起来，如下图所示：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/09.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;黑色表示当前账簿主干。此时，可以随便选择一个页作为当前主分支，例如选择A：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/10.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;此时如果有一个新的账簿页是基于A的，那么这个主干就延续下去：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/11.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;如果这个主干一直这么延续下去，表示大家基本都以A为主干，B就会被遗忘。但是也有可能忽然B变成更长了：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/12.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;那么我们就需要将B分支作为当前主干，基于这个分支进行后续工作。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/13.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;从局部来看，虽然在某一时刻各个小组的账簿主干可能存在不一致，但大方向是一致的，那些偶尔由于不同步产生的小分支，会很快被淹没在历史中。&lt;/p&gt;
&lt;h3 id=&quot;如果挖矿小组有人伪造账簿怎么办&quot;&gt;如果挖矿小组有人伪造账簿怎么办&lt;/h3&gt;
&lt;p&gt;关于这个问题，中本聪同学说，只要挖矿组织中大多数人是诚实的，这个系统就可靠，具体分几个方面给予答复。&lt;/p&gt;
&lt;p&gt;首先，基于保密印章机制，没有人能伪造他人身份进行付款，因为编码生成器在打印编码时会核对所有交易单的保密印章，印章和付款人不一致会拒绝打印。&lt;/p&gt;
&lt;p&gt;而且诚实的矿工也不会承认不合法的交易（如某笔交易付款方余额不够）。&lt;/p&gt;
&lt;p&gt;所以只有一种可能的攻击行为，即在收款人确认收款后，从另一条分支上建立另外的交易单，取消之前的付款，而将同一笔钱再次付款给另一个人（即所谓的double-spending问题）。下面同样用一个例子说明这个问题。&lt;/p&gt;
&lt;p&gt;先假设有一个攻击者拥有10个比特币，他准备将这笔钱同时支付给两名受害者A和B，并都得到承认。&lt;/p&gt;
&lt;p&gt;第一步，攻击者准备从受害者A手里买10比特币的黄金，他签署交易单给受害者A，转10个比特币给受害者A。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/14.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;第二步，这笔交易在最新的账簿页中被确认，并被各个挖矿小组公告出来。受害人A看到公告，确认比特币到账，给了攻击者10个比特币等值的黄金。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/15.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;第三步，攻击者找到账簿，从包含刚才交易的账簿页的前一页做出一个分支，生成更多的账单页，超过刚才的分支。由于此时刚才攻击者制造的分支变成了主干分支，而包含受害者A得到钱的分支变成了旁支，因此挖矿组织不再承认刚才的转账，受害者A得到的10比特币被取消了。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/16.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;第四步，攻击者可以再次签署交易单，将同一笔钱支付给受害者B。受害者B确认钱到账后，支付给攻击者等值黄金。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/bitcoin-mechanism-make-easy/17.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;至此，攻击者将10个比特币花了两次，从两名受害者那里各购得等值黄金。攻击者还可以如法炮制，取消与受害者B的转账，将同一笔钱再支付给其他人……&lt;/p&gt;
&lt;p&gt;关于这种攻击，中本聪给出的解决方案是，建议收款人不要在公告挂出时立即确认交易完成，而是应该再看一段时间，等待各个挖矿小组再挂出6张确认账簿，并且之前的账簿没有被取消，才确认钱已到账。&lt;/p&gt;
&lt;p&gt;中本聪解释道，之前设定变态的编号规则，正是为了防御这一点。根据前面所述，生成有效账簿页不是那么简单的，要花费大量的人力反复试不同的幸运数字，而且过程完全是碰运气。如果某账簿页包含你收到钱的确认，并且在后面又延续了6个，那么攻击者想要在落后6页的情况下从另一个分支赶超当前主分支是非常困难的，除非攻击者拥有非常多的人力，超过其他所有诚实矿工的人力之和。&lt;/p&gt;
&lt;p&gt;而且，如果攻击者有如此多人力，与其花这么大力气搞这种攻击，还不如做良民挖矿来的收益大。这就从动机上杜绝了攻击的形成。&lt;/p&gt;
&lt;h3 id=&quot;比特币会一直增加下去，岂不是会严重通货膨胀&quot;&gt;比特币会一直增加下去，岂不是会严重通货膨胀&lt;/h3&gt;
&lt;p&gt;中本聪说，这一点我也想到了。前面忘了说了，我给矿工组织的操作细则手册会说明，刚开始我们协议每生成一页账簿，奖励小组50个比特币，后面，每当账簿增加21,000页，奖励就减半，例如当达到210,000页后，每生成一页账簿奖励25个比特币，420,000页后，每生成一页奖励12.5个，依次类推，等账簿达到6,930,000页后，新生成账簿页就没有奖励了。此时比特币全量约为21,000,000个，这就是比特币的总量，所以不会无限增加下去。&lt;/p&gt;
&lt;h3 id=&quot;没有奖励后，就没人做矿工了，岂不是没人帮忙确认交易了&quot;&gt;没有奖励后，就没人做矿工了，岂不是没人帮忙确认交易了&lt;/h3&gt;
&lt;p&gt;到时，矿工的收益会由挖矿所得变为收取手续费。例如，你在转账时可以指定其中1%作为手续费支付给生成账簿页的小组，各个小组会挑选手续费高的交易单优先确认。&lt;/p&gt;
&lt;h3 id=&quot;矿工如果越来越多，比特币生成速度会变快吗&quot;&gt;矿工如果越来越多，比特币生成速度会变快吗&lt;/h3&gt;
&lt;p&gt;不会。中本聪解释，虽然可以任意加入和退出矿工组织，导致矿工人数变化，每个矿工也会拿到一个编码生成器，不过我已经在编码生成器中加入了调控机制，当前工作的编码生成器越多，每个机器的效率就越低，保证新账簿页生成速率不变。&lt;/p&gt;
&lt;h3 id=&quot;虽然每个人的代号是匿名的，但如果泄露了某个人的代号，账簿又是公开的，岂不是他的所有账目都查出来了&quot;&gt;虽然每个人的代号是匿名的，但如果泄露了某个人的代号，账簿又是公开的，岂不是他的所有账目都查出来了&lt;/h3&gt;
&lt;p&gt;确实是这样的。例如你要和某人交易，必然要要到他的代号才能填写交易单。因为收款人一栏要填入那人的代号。不过中本聪说可以提供无限制的保密印章，建议每一次交易用不同的保密印章，这样查账簿就追查不到同一个人的所有账目了。&lt;/p&gt;
&lt;p&gt;答疑完毕。&lt;/p&gt;
&lt;h1 id=&quot;说明&quot;&gt;说明&lt;/h1&gt;
&lt;p&gt;本文用通俗比喻的方式讲解了比特币的运行机制。有几点需要说明：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为了便于理解，我做了很多简化，因此有些机制细节和实际的比特币可能不完全相同。但总体思想和关键原理是一致的。&lt;/li&gt;
&lt;li&gt;由于很多计算机世界的东西（如公钥体系、网络传输）在现实世界中并没有特别好的对等物，所以故事里难免有一些生硬和不合常理的细节。&lt;/li&gt;
&lt;li&gt;本文描述的是比特币网络本身的技术原理和运作机制，当在如Mtgox这种买卖市场中进行比特币交易时，市场做了中间代理，并不遵从上述机制&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://bitcoin.org/bitcoin.pdf&quot;&gt;Bitcoin: A Peer-to-Peer Electronic Cash System&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://bitcoin.it&quot;&gt;https://bitcoin.it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codingnow.com/2011/05/bitcoin.html&quot;&gt;云风的BLOG: Bitcoin 的基本原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.btc123.com/data/docs/easy_understood_bitcoin_mechanism.pdf&quot;&gt;易懂的比特币工作机理详解&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description> 
        </item> 
        
        <item> 
            <title>MySQL索引与Index Condition Pushdown</title> 
            <link>http://blog.codinglabs.org/articles/index-condition-pushdown.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/index-condition-pushdown.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Thu, 05 Dec 2013 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;大约在两年前，我写了&lt;a href=&quot;/articles/theory-of-mysql-index.html&quot;&gt;一篇关于MySQL索引的文章&lt;/a&gt;。最近有同学在文章的评论中对文章的内容提出质疑，质疑主要集中在联合索引的使用方式上。在那篇文章中，我说明联合索引是将各个索引字段做字符串连接后作为key，使用时将整体做前缀匹配。&lt;/p&gt;
&lt;p&gt;而这名同学在&lt;a href=&quot;https://mariadb.com/kb/en/index-condition-pushdown/&quot;&gt;这个页面&lt;/a&gt;找到了如下一句话：index condition pushdown is usually useful with multi-column indexes: the first component(s) is what index access is done for, the subsequent have columns that we read and check conditions on。从而认为联合索引的使用方式与文中不符。&lt;/p&gt;
&lt;p&gt;实际上，这个页面所讲述的是在MariaDB 5.3.3（MySQL是在5.6）开始引入的一种叫做Index Condition Pushdown（以下简称ICP）的查询优化方式。由于本身不是一个层面的东西，前文中说的是Index Access，而这里是Query Optimization，所以并不构成对前文正确性的影响。在写前文时，MySQL还没有ICP，所以文中没有涉及相关内容，但考虑到新版本的MariaDB或MySQL中ICP的启用确实影响了一些查询行为的外在表现。所以决定写这篇文章详细讲述一下ICP的原理以及对索引使用方式的优化。&lt;/p&gt;
&lt;h1 id=&quot;实验&quot;&gt;实验&lt;/h1&gt;
&lt;p&gt;先从一个简单的实验开始直观认识ICP的作用。&lt;/p&gt;
&lt;h2 id=&quot;安装数据库&quot;&gt;安装数据库&lt;/h2&gt;
&lt;p&gt;首先需要安装一个支持ICP的MariaDB或MySQL数据库。我使用的是MariaDB 5.5.34，如果是使用MySQL则需要5.6版本以上。&lt;/p&gt;
&lt;p&gt;Mac环境下可以通过brew安装：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;brew install mairadb
&lt;/pre&gt;
&lt;p&gt;其它环境下的安装请参考&lt;a href=&quot;https://www.mariadb.com/kb/en/getting-installing-and-upgrading-mariadb/&quot;&gt;MariaDB官网关于下载安装的文档&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;导入示例数据&quot;&gt;导入示例数据&lt;/h2&gt;
&lt;p&gt;与前文一样，我们使用&lt;a href=&quot;https://launchpad.net/test-db/&quot;&gt;Employees Sample Database&lt;/a&gt;，作为示例数据库。完整示例数据库的下载地址为：&lt;a href=&quot;https://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2&quot;&gt;https://launchpad.net/test-db/employees-db-1/1.0.6/+download/employees_db-full-1.0.6.tar.bz2&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;将下载的压缩包解压后，会看到一系列的文件，其中employees.sql就是导入数据的命令文件。执行&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-bash&quot;&gt;mysql -h[host] -u[user] -p &lt; employees.sql
&lt;/pre&gt;
&lt;p&gt;就可以完成建库、建表和load数据等一系列操作。此时数据库中会多一个叫做employees的数据库。库中的表如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;MariaDB [employees]&gt; SHOW TABLES;
+---------------------+
| Tables_in_employees |
+---------------------+
| departments         |
| dept_emp            |
| dept_manager        |
| employees           |
| salaries            |
| titles              |
+---------------------+
6 rows in set (0.00 sec)
&lt;/pre&gt;
&lt;p&gt;我们将使用employees表做实验。&lt;/p&gt;
&lt;h2 id=&quot;建立联合索引&quot;&gt;建立联合索引&lt;/h2&gt;
&lt;p&gt;employees表包含雇员的基本信息，表结构如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;MariaDB [employees]&gt; DESC employees.employees;
+------------+---------------+------+-----+---------+-------+
| Field      | Type          | Null | Key | Default | Extra |
+------------+---------------+------+-----+---------+-------+
| emp_no     | int(11)       | NO   | PRI | NULL    |       |
| birth_date | date          | NO   |     | NULL    |       |
| first_name | varchar(14)   | NO   |     | NULL    |       |
| last_name  | varchar(16)   | NO   |     | NULL    |       |
| gender     | enum(&#39;M&#39;,&#39;F&#39;) | NO   |     | NULL    |       |
| hire_date  | date          | NO   |     | NULL    |       |
+------------+---------------+------+-----+---------+-------+
6 rows in set (0.01 sec)
&lt;/pre&gt;
&lt;p&gt;这个表默认只有一个主索引，因为ICP只能作用于二级索引，所以我们建立一个二级索引：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;ALTER TABLE employees.employees ADD INDEX first_name_last_name (first_name, last_name);
&lt;/pre&gt;
&lt;p&gt;这样就建立了一个first_name和last_name的联合索引。&lt;/p&gt;
&lt;h2 id=&quot;查询&quot;&gt;查询&lt;/h2&gt;
&lt;p&gt;为了明确看到查询性能，我们启用profiling并关闭query cache：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;SET profiling = 1;
SET query_cache_type = 0;
SET GLOBAL query_cache_size = 0;
&lt;/pre&gt;
&lt;p&gt;然后我们看下面这个查询：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;MariaDB [employees]&gt; SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39;;
+--------+------------+------------+-----------+--------+------------+
| emp_no | birth_date | first_name | last_name | gender | hire_date  |
+--------+------------+------------+-----------+--------+------------+
| 254642 | 1959-01-17 | Mary       | Botman    | M      | 1989-11-24 |
| 471495 | 1960-09-24 | Mary       | Dymetman  | M      | 1988-06-09 |
| 211941 | 1962-08-11 | Mary       | Hofman    | M      | 1993-12-30 |
| 217707 | 1962-09-05 | Mary       | Lichtman  | F      | 1987-11-20 |
| 486361 | 1957-10-15 | Mary       | Oberman   | M      | 1988-09-06 |
| 457469 | 1959-07-15 | Mary       | Weedman   | M      | 1996-11-21 |
+--------+------------+------------+-----------+--------+------------+
&lt;/pre&gt;
&lt;p&gt;根据MySQL索引的前缀匹配原则，两者对索引的使用是一致的，即只有first_name采用索引，last_name由于使用了模糊前缀，没法使用索引进行匹配。我将查询联系执行三次，结果如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;+----------+------------+---------------------------------------------------------------------------+
| Query_ID | Duration   | Query                                                                     |
+----------+------------+---------------------------------------------------------------------------+
|       38 | 0.00084400 | SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39; |
|       39 | 0.00071800 | SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39; |
|       40 | 0.00089600 | SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39; |
+----------+------------+---------------------------------------------------------------------------+
&lt;/pre&gt;
&lt;p&gt;然后我们关闭ICP：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;SET optimizer_switch=&#39;index_condition_pushdown=off&#39;;
&lt;/pre&gt;
&lt;p&gt;在运行三次相同的查询，结果如下：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;+----------+------------+---------------------------------------------------------------------------+
| Query_ID | Duration   | Query                                                                     |
+----------+------------+---------------------------------------------------------------------------+
|       42 | 0.00264400 | SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39; |
|       43 | 0.01418900 | SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39; |
|       44 | 0.00234200 | SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39; |
+----------+------------+---------------------------------------------------------------------------+
&lt;/pre&gt;
&lt;p&gt;有意思的事情发生了，关闭ICP后，同样的查询，耗时是之前的三倍以上。下面我们用explain看看两者有什么区别：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;MariaDB [employees]&gt; EXPLAIN SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39;;
+------+-------------+-----------+------+----------------------+----------------------+---------+-------+------+-----------------------+
| id   | select_type | table     | type | possible_keys        | key                  | key_len | ref   | rows | Extra                 |
+------+-------------+-----------+------+----------------------+----------------------+---------+-------+------+-----------------------+
|    1 | SIMPLE      | employees | ref  | first_name_last_name | first_name_last_name | 44      | const |  224 | Using index condition |
+------+-------------+-----------+------+----------------------+----------------------+---------+-------+------+-----------------------+
1 row in set (0.00 sec)
&lt;/pre&gt;
&lt;pre class=&quot;prettyprint linenums lang-sql&quot;&gt;MariaDB [employees]&gt; EXPLAIN SELECT * FROM employees WHERE first_name=&#39;Mary&#39; AND last_name LIKE &#39;%man&#39;;
+------+-------------+-----------+------+----------------------+----------------------+---------+-------+------+-------------+
| id   | select_type | table     | type | possible_keys        | key                  | key_len | ref   | rows | Extra       |
+------+-------------+-----------+------+----------------------+----------------------+---------+-------+------+-------------+
|    1 | SIMPLE      | employees | ref  | first_name_last_name | first_name_last_name | 44      | const |  224 | Using where |
+------+-------------+-----------+------+----------------------+----------------------+---------+-------+------+-------------+
1 row in set (0.00 sec)
&lt;/pre&gt;
&lt;p&gt;前者是开启ICP，后者是关闭ICP。可以看到区别在于Extra，开启ICP时，用的是Using index condition；关闭ICP时，是Using where。&lt;/p&gt;
&lt;p&gt;其中Using index condition就是ICP提高查询性能的关键。下一节说明ICP提高查询性能的原理。&lt;/p&gt;
&lt;h1 id=&quot;原理&quot;&gt;原理&lt;/h1&gt;
&lt;p&gt;ICP的原理简单说来就是将可以利用索引筛选的where条件在存储引擎一侧进行筛选，而不是将所有index access的结果取出放在server端进行where筛选。&lt;/p&gt;
&lt;p&gt;以上面的查询为例，在没有ICP时，首先通过索引前缀从存储引擎中读出224条first_name为Mary的记录，然后在server段用where筛选last_name的like条件；而启用ICP后，由于last_name的like筛选可以通过索引字段进行，那么存储引擎内部通过索引与where条件的对比来筛选掉不符合where条件的记录，这个过程不需要读出整条记录，同时只返回给server筛选后的6条记录，因此提高了查询性能。&lt;/p&gt;
&lt;p&gt;下面通过图两种查询的原理详细解释。&lt;/p&gt;
&lt;h2 id=&quot;关闭icp&quot;&gt;关闭ICP&lt;/h2&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/index-condition-pushdown/01.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;在不支持ICP的系统下，索引仅仅作为data access使用。&lt;/p&gt;
&lt;h2 id=&quot;开启icp&quot;&gt;开启ICP&lt;/h2&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/index-condition-pushdown/02.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;在ICP优化开启时，在存储引擎端首先用索引过滤可以过滤的where条件，然后再用索引做data access，被index condition过滤掉的数据不必读取，也不会返回server端。&lt;/p&gt;
&lt;h2 id=&quot;注意事项&quot;&gt;注意事项&lt;/h2&gt;
&lt;p&gt;有几个关于ICP的事情要注意：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ICP只能用于二级索引，不能用于主索引。&lt;/li&gt;
&lt;li&gt;也不是全部where条件都可以用ICP筛选，如果某where条件的字段不在索引中，当然还是要读取整条记录做筛选，在这种情况下，仍然要到server端做where筛选。&lt;/li&gt;
&lt;li&gt;ICP的加速效果取决于在存储引擎内通过ICP筛选掉的数据的比例。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://mariadb.com/kb/en/index-condition-pushdown/&quot;&gt;https://mariadb.com/kb/en/index-condition-pushdown/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.6/en/index-condition-pushdown-optimization.html&quot;&gt;http://dev.mysql.com/doc/refman/5.6/en/index-condition-pushdown-optimization.html&lt;/a&gt;&lt;/p&gt;
</description> 
        </item> 
        
        <item> 
            <title>使用MPlayer观看Coursera课程视频的一些心得</title> 
            <link>http://blog.codinglabs.org/articles/using-mplayer-for-coursera.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/using-mplayer-for-coursera.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Sun, 06 Oct 2013 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;之前一直是在线看&lt;a href=&quot;https://www.coursera.org/&quot;&gt;Coursera&lt;/a&gt;上的课程视频。最近迫于租住的房子网速太差，加之Coursera访问经常不稳定，为了使得流畅学习的过程不被破坏，开始考虑将视频下载到本地观看。&lt;/p&gt;
&lt;p&gt;因为之前一直没有在本地看视频的习惯，很少使用播放器，所以找个顺心的播放器就成了重中之重。经过一番折腾，最终选择了&lt;a href=&quot;http://www.mplayerhq.hu&quot;&gt;MPlayer&lt;/a&gt;，原因主要有如下几点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;免费的。&lt;/li&gt;
&lt;li&gt;MPlayer同时支持Mac和Linux。因为我公司用的是Mac，而住处用的是Linux，所以播放器能跨这两个操作系统很重要。&lt;/li&gt;
&lt;li&gt;通过命令行操作，简单快捷。这也是很吸引我的一点，我实在不习惯为了播放一个视频用鼠标在文件系统中一层一层的点啊点，而MPlayer只要一行命令就可以播放指定视频，并且可以方便的指定播放参数。播放过程中也是全快捷键操作，效率很高。&lt;/li&gt;
&lt;li&gt;能自动加载srt字幕。英文课程还是需要字幕支持的。&lt;/li&gt;
&lt;li&gt;可以快放并且保持语调不变。这点对我也很重要，我看视频一般是用1.5倍速播放，Coursera的html5播放器可以完美支持快放且保持音调不变，而我惊喜的发现MPlayer也只要通过简单的启动参数就可以实现相同的效果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面和大家分享一下用MPlayer看Coursera视频的心得。&lt;/p&gt;
&lt;h1 id=&quot;安装&quot;&gt;安装&lt;/h1&gt;
&lt;p&gt;MPlayer存在于大多数Linux发行版和Mac的软件源里，因此可以很方便的安装。&lt;/p&gt;
&lt;p&gt;Mac下建议用&lt;a href=&quot;http://brew.sh/&quot;&gt;Homebrew&lt;/a&gt;安装：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
brew install mplayer
&lt;/pre&gt;

&lt;p&gt;注意，如果你和我一样用的是OSX 10.9 Mavericks，那么直接安装会失败。这是因为brew的mplayer安装脚本不兼容10.9，这块我折腾了好久。最后在github上找到了&lt;a href=&quot;https://github.com/i8degrees/homebrew/commit/d0ba78cf321bb7fa005284377e50e98d57bf13a7&quot;&gt;相关的补丁&lt;/a&gt;。这个补丁目前还在brew的hotfix分支下，没有合并到master，我们可以手工应用补丁，将这个补丁文件覆盖掉我们本地brew下的mplayer.rb文件，再执行上面的命令就可以正常安装了。&lt;/p&gt;
&lt;p&gt;Linux下可以用相应发行版的软件源安装。例如Ubuntu或Linux Mint（我使用的发行版），可以通过apt安装：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
sudo apt-get install mplayer
&lt;/pre&gt;

&lt;h1 id=&quot;使用&quot;&gt;使用&lt;/h1&gt;
&lt;h2 id=&quot;打开文件&quot;&gt;打开文件&lt;/h2&gt;
&lt;p&gt;安装完成后，直接在命令行用：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
mplayer 视频文件
&lt;/pre&gt;

&lt;p&gt;就可以打开相应的文件进行播放了，具体效果见下图：&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/using-mplayer-for-coursera/01.png&quot;/&gt;&lt;/p&gt;

&lt;h2 id=&quot;基本操作&quot;&gt;基本操作&lt;/h2&gt;
&lt;p&gt;MPlayer的播放界面只有一个画面框，所有操作都是通过键盘完成。基本操作如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;左右箭头 - 快退或快进10秒&lt;/li&gt;
&lt;li&gt;上下箭头 - 快退或快进1分钟&lt;/li&gt;
&lt;li&gt;PageUp和PageDown - 快进或快退10分钟&lt;/li&gt;
&lt;li&gt;p或空格键 - 暂停/继续&lt;/li&gt;
&lt;li&gt;q或ESC键 - 退出播放&lt;/li&gt;
&lt;li&gt;/和* - 减小或增大音量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其它还有一些常用操作可以通过以下命令查看：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
mplayer -h
&lt;/pre&gt;

&lt;h2 id=&quot;挂载字幕&quot;&gt;挂载字幕&lt;/h2&gt;
&lt;p&gt;如果有和视频文件同名的srt文件，MPlayer会自动挂载字幕，也可以通过-sub来指定字幕：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
mplayer -sub 字幕文件 视频文件
&lt;/pre&gt;

&lt;h2 id=&quot;加速播放&quot;&gt;加速播放&lt;/h2&gt;
&lt;p&gt;大多数课程视频讲解稍显啰嗦，因此可以通过加速播放节省学习时间。MPlayer通过-speed选项改变播放速度，例如：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
mplayer -speed 1.5 path/to/video/file
&lt;/pre&gt;

&lt;p&gt;可以将播放速度调整为原始的1.5倍。&lt;/p&gt;
&lt;h2 id=&quot;保持正常音调&quot;&gt;保持正常音调&lt;/h2&gt;
&lt;p&gt;用上面的方法加速播放后，声音也会加速，频率变快，因此音调非常尖锐刺耳，这个问题可以通过ScaleTempo插件解决。这个插件默认包含在MPlayer中。只要通过下面命令：&lt;/p&gt;
&lt;pre class=&quot;prettyprint linenums&quot;&gt;
mplayer -af scaletempo -speed 1.5 path/to/video/file
&lt;/pre&gt;

&lt;p&gt;就可以加速播放的同时保持语调不变了。&lt;/p&gt;
</description> 
        </item> 
        
        <item> 
            <title>五种常用基数估计算法效果实验及实践建议</title> 
            <link>http://blog.codinglabs.org/articles/cardinality-estimate-exper.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/cardinality-estimate-exper.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Fri, 30 Aug 2013 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;之前我曾写过&lt;a href=&quot;http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html&quot;&gt;一系列关于基数估计（cardinality estimation）算法的文章&lt;/a&gt;，文中介绍了一些常用基数估计算法的原理。最近对常用的基数估计算法做了一些实验，这篇文章描述了实验结果，包括这些算法的估计效果及误差状况，主要通过图表展示。通过观察实验数据和可视化图表可以加强对各种基数估计算法理论分析的直观理解。&lt;/p&gt;
&lt;p&gt;文章首先会对实验做一些说明，然后通过图表详细展示实验数据，最后会根据实验结果总结一些实践中有用的结论。&lt;/p&gt;
&lt;h1 id=&quot;实验说明&quot;&gt;实验说明&lt;/h1&gt;
&lt;h2 id=&quot;算法选择&quot;&gt;算法选择&lt;/h2&gt;
&lt;p&gt;这次实验共选择了五种基数估计算法，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Counting&lt;a href=&quot;#ref1&quot;&gt;&lt;sup&gt;1&lt;sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LogLog Counting&lt;a href=&quot;#ref2&quot;&gt;&lt;sup&gt;2&lt;sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive Counting&lt;a href=&quot;#ref3&quot;&gt;&lt;sup&gt;3&lt;sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HyperLogLog Counting&lt;a href=&quot;#ref4&quot;&gt;&lt;sup&gt;4&lt;sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HyperLogLog++ Counting&lt;a href=&quot;#ref5&quot;&gt;&lt;sup&gt;5&lt;sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;算法实现使用我所在部门（阿里巴巴商家数据部）的开源基数估计算法库&lt;a href=&quot;https://github.com/chaoslawful/ccard-lib&quot;&gt;ccard-lib&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;数据准备&quot;&gt;数据准备&lt;/h2&gt;
&lt;p&gt;哈希函数采用&lt;a href=&quot;http://en.wikipedia.org/wiki/MurmurHash&quot;&gt;murmurhash32&lt;/a&gt;（HyperLogLog++采用murmurhash64）。&lt;/p&gt;
&lt;p&gt;因实验结果的可靠性仅与哈希值的分布均匀性有关，而根据之前相关研究&lt;a href=&quot;http://programmers.stackexchange.com/questions/49550/which-hashing-algorithm-is-best-for-uniqueness-and-speed&quot;&gt;murmurhash对于顺序型数据具有良好的均匀性&lt;/a&gt;。因此为了简化实验，原始数据使用&lt;em&gt;1-1,000,000无符号64bit整型的小端序表示&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;下面将通过实验验证原始数据哈希后的均匀性。&lt;/p&gt;
&lt;h2 id=&quot;实验过程&quot;&gt;实验过程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;将原始数据经过murmurhash处理后，验证分桶数在\(2^{10}\)，\(2^{12}\)和\(2^{16}\)下数据的均匀性，即看各个桶的元素数量是否大致相等；同时验证各个桶中元素二进制表示的最长0前缀是否服从幂率分布。&lt;/li&gt;
&lt;li&gt;对五种基数估计算法，分布记录\(2^{10}\)，\(2^{12}\)和\(2^{16}\)三种分桶数量下从1到1,000,000的估计值和相对误差值。取样点为100的整倍数，因此共10,000个采样点。&lt;/li&gt;
&lt;li&gt;比较在\(2^{10}\)，\(2^{12}\)和\(2^{16}\)三种分桶数量下五种基数估计算法的误差走势。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;实验&quot;&gt;实验&lt;/h1&gt;
&lt;h2 id=&quot;数据均匀性&quot;&gt;数据均匀性&lt;/h2&gt;
&lt;p&gt;下面首先验证原始数据经过哈希后基本服从均匀分布，从而满足各种基数估计算法的基本前提条件。下面的结果通过murmurhash32哈希值给出，实际中采用murmurhash64得到了基本一致的结论。&lt;/p&gt;
&lt;p&gt;对于32bit哈希值，分桶数为\(2^p\)时，用前\(p\)bit作为桶编号，剩下的\(32-p\)作为用于统计0后缀（因为均匀分布的假设，统计0后缀和0前缀是等效的，ccard-lib中除HyperLogLog++外采用统计0后缀的方式）的比特串。例如对于哈希值“01001010111010100101000000100100”，分桶数为\(2^{10}\)时，其桶编号为“0100101011”，即十进制的“555”，剩余部分为“1010100101000000100100”，零后缀长度为2。&lt;/p&gt;
&lt;h3 id=&quot;验证分桶均匀性&quot;&gt;验证分桶均匀性&lt;/h3&gt;
&lt;p&gt;下面通过柱状图分别给出\(2^{10}\)，\(2^{12}\)和\(2^{16}\)三种分桶下各桶元素数量的分布，在柱状图中bins的数量均为100，因此图中每个bin并不对应一个桶。&lt;/p&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;murmurhash32哈希值分布（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/murmurhash32-10-distrib.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;murmurhash32哈希值分布（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/murmurhash32-12-distrib.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;murmurhash32哈希值分布（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/murmurhash32-16-distrib.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;可以看到，三种分桶下数据均基本服从均匀分布。&lt;/p&gt;
&lt;h3 id=&quot;0后缀长度的幂率分布性&quot;&gt;0后缀长度的幂率分布性&lt;/h3&gt;
&lt;p&gt;按照理论预言，如果哈希均匀性足够好，哈希剩余部分的关键统计量（最长0后缀长度）应该大约服从底数为2的幂率分布。&lt;/p&gt;
&lt;p&gt;下图中横坐标表示0后缀长度，纵坐标表示0后缀为此长度的哈希值个数。&lt;/p&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;0后缀长度分布（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/murmurhash32-10-buckets.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;0后缀长度分布（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/murmurhash32-12-buckets.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;0后缀长度分布（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/murmurhash32-16-buckets.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;可以看到在三种分桶下统计量分布符合预期。&lt;/p&gt;
&lt;p&gt;通过以上分析可知实验数据满足基数估计算法关于均匀性的假设。&lt;/p&gt;
&lt;h2 id=&quot;基数估计算法效果&quot;&gt;基数估计算法效果&lt;/h2&gt;
&lt;p&gt;下面给出五种基数估计算法的估计效果和误差走势。如未特殊说明，实验分桶数均为\(2^{10}\)，\(2^{12}\)和\(2^{16}\)。&lt;/p&gt;
&lt;h3 id=&quot;linear-counting&quot;&gt;Linear Counting&lt;/h3&gt;
&lt;p&gt;ccard-lib中当单独使用Linear Counting时，采用bit为单位记录哈希结果。因此实际的精度为分桶数的8倍，例如\(2^{10}\)时，实际的精度为1024*8=8192。&lt;/p&gt;
&lt;h4 id=&quot;估计效果&quot;&gt;估计效果&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Linear Counting（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/lc-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Linear Counting（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/lc-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Linear Counting（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/lc-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;相对误差&quot;&gt;相对误差&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Linear Counting误差（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/lc-error-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Linear Counting误差（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/lc-error-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Linear Counting误差（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/lc-error-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;结论&quot;&gt;结论&lt;/h4&gt;
&lt;p&gt;如理论预期，由于Linear Counting的有效性取决于bitmap中存在空位置，当有位置留空时，估计效果还不错，但是当bitmap全满后，Linear Counting完全失效。Linear Counting的有效估计范围线性依赖于bitmap长度。&lt;/p&gt;
&lt;h3 id=&quot;loglog-counting&quot;&gt;LogLog Counting&lt;/h3&gt;
&lt;h4 id=&quot;估计效果&quot;&gt;估计效果&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;相对误差&quot;&gt;相对误差&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting误差（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-error-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting误差（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-error-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting误差（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-error-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;结论&quot;&gt;结论&lt;/h4&gt;
&lt;p&gt;LogLog Counting的表现基本与理论相符，可以看到当基数不太大的时候，LogLog Counting误差非常大，这是因为LogLog Counting在基数较小的段存在一个很大的偏差。为了明确看到这个偏差，我们截取前十分之一放大，也就是1-100,000这一段的效果图：&lt;/p&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting小基数区间（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-10-small-range.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting小基数区间（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-12-small-range.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;LogLog Counting小基数区间（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/llc-16-small-range.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;可以很明显的看到估计值严重偏离基准，而且分桶数越多这个偏差反而越明显。&lt;/p&gt;
&lt;h3 id=&quot;adaptive-counting&quot;&gt;Adaptive Counting&lt;/h3&gt;
&lt;h4 id=&quot;估计效果&quot;&gt;估计效果&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Adaptive Counting（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/ac-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Adaptive Counting（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/ac-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Adaptive Counting（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/ac-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;相对误差&quot;&gt;相对误差&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Adaptive Counting误差（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/ac-error-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Adaptive Counting误差（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/ac-error-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;Adaptive Counting误差（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/ac-error-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;结论&quot;&gt;结论&lt;/h4&gt;
&lt;p&gt;由于分别在基数较小和较大时使用Linear Counting和LogLog Counting，Adaptive Counting克服了两者的缺陷，属于比较稳定的基数估计方法。而且随着分桶数的增加，估计的偏差和方差均明显减小。&lt;/p&gt;
&lt;h3 id=&quot;hyperloglog-counting&quot;&gt;HyperLogLog Counting&lt;/h3&gt;
&lt;h4 id=&quot;估计效果&quot;&gt;估计效果&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog Counting（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllc-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog Counting（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllc-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog Counting（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllc-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;相对误差&quot;&gt;相对误差&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog Counting误差（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllc-error-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog Counting误差（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllc-error-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog Counting误差（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllc-error-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;结论&quot;&gt;结论&lt;/h4&gt;
&lt;p&gt;HyperLogLog Counting采用调和平均数取代LogLog Counting中的几何平均数，旨在减小离群点的影响，并且对Linear Counting转折阈值做了调整。从实验效果看，在分桶数较小时，改进效果并不明显，不过在\(2^{16}\)分桶下，整体偏差和稳定程度优于Adaptive Counting。&lt;/p&gt;
&lt;p&gt;但是从误差图中可以看到，在200,000附近出现了一个明显的脉冲。其原因在Google关于HyperLogLog++ Counting的论文中&lt;a href=&quot;#ref5&quot;&gt;&lt;sup&gt;5&lt;sup&gt;&lt;/a&gt;有分析，其主要是因为在Linear Counting刚转折后的一小段区域内存在一个偏差，HyperLogLog++ Counting的一个改进就是对这个区域的偏差进行了修正。&lt;/p&gt;
&lt;h3 id=&quot;hyperloglog-counting&quot;&gt;HyperLogLog++ Counting&lt;/h3&gt;
&lt;h4 id=&quot;估计效果&quot;&gt;估计效果&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog++ Counting（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllpc-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog++ Counting（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllpc-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog++ Counting（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllpc-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;相对误差&quot;&gt;相对误差&lt;/h4&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog++ Counting误差（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllpc-error-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog++ Counting误差（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllpc-error-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;HyperLogLog++ Counting误差（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/hllpc-error-16.png&quot;/&gt;&lt;/p&gt;

&lt;h4 id=&quot;结论&quot;&gt;结论&lt;/h4&gt;
&lt;p&gt;可以看到HyperLogLog++ Counting的效果非常令人失望，按论文中说法，HyperLogLog++ Counting应该比HyperLogLog Counting更准确，但实际效果不但整体偏差和方差变大，而且偏差修正的阈值明显有问题，导致一个非常明显的误差脉冲。&lt;/p&gt;
&lt;p&gt;究其原因，个人认为HyperLogLog++ Counting中的偏差修正和转折阈值均是通过统计方法给出，并不是数学上的解析结果，因此对于不同的数据、不同的哈希可能并不通用。&lt;/p&gt;
&lt;h2 id=&quot;误差比较&quot;&gt;误差比较&lt;/h2&gt;
&lt;p&gt;为了更清楚对比五种算法的误差情况，下面给出五种算法的误差曲线叠加图，仍然是采用三个分桶数。&lt;/p&gt;
&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;误差对比（p=10）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/errors-10.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;误差对比（p=12）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/errors-12.png&quot;/&gt;&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;strong&gt;误差对比（p=16）&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/cardinality-estimate-exper/errors-16.png&quot;/&gt;&lt;/p&gt;

&lt;h1 id=&quot;实践建议&quot;&gt;实践建议&lt;/h1&gt;
&lt;p&gt;下面根据实验结果从个人角度给出一些基数估计算法的实践性建议，当然只代表个人意见，不同人对实验结果可能有不同解读。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Counting和LogLog Counting由于分别在基数较大和基数较小（阈值可解析分析，具体方法和公式请参考后文列出的相关论文）时存在严重的失效，因此不适合在实际中单独使用。一种例外是，如果对节省存储空间要求不强烈，不要求空间复杂度为常数（Linear Counting的空间复杂度为\(O(n)\)，其它算法均为\(O(1)\)），则在保证bitmap全满概率很小的条件下，Linear Counting的效果要优于其它算法。&lt;/li&gt;
&lt;li&gt;总体来看，不论哪种算法，提高分桶数都可以降低偏差和方差，因此总体来看基数估计算法中分桶数的选择是最重要的一个权衡——在精度和存储空间间的权衡。&lt;/li&gt;
&lt;li&gt;实际中，Adaptive Counting或HyperLogLog Counting都是不错的选择，前者偏差较小，后者对离群点容忍性更好，方差较小。&lt;/li&gt;
&lt;li&gt;Google的HyperLogLog Counting++算法属于实验性改进，缺乏严格的数学分析基础，通用性存疑，不宜在实际中贸然使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;参考文献&quot;&gt;参考文献&lt;/h1&gt;
&lt;p&gt;&lt;a id=&quot;ref1&quot;&gt;&lt;/a&gt;
[1] K.-Y. Whang, B. T. Vander-Zanden, and H. M. Taylor. A Linear-Time Probabilistic Counting Algorithm for Database Applications. ACM Transactions on Database Systems, 15(2):208-229, 1990.&lt;/p&gt;
&lt;p&gt;&lt;a id=&quot;ref2&quot;&gt;&lt;/a&gt;
[2] Marianne Durand and Philippe Flajolet. LogLog counting of large cardinalities. In ESA03, volume 2832 of LNCS, pages 605b 617, 2003.&lt;/p&gt;
&lt;p&gt;&lt;a id=&quot;ref3&quot;&gt;&lt;/a&gt;
[3] Min Cai, Jianping Pan, Yu K. Kwok, and Kai Hwang. Fast and accurate traffic matrix measurement using adaptive cardinality counting. In MineNet b 05: Proceedings of the 2005 ACM SIGCOMM workshop on Mining network data, pages 205b 206, New York, NY, USA, 2005. ACM.&lt;/p&gt;
&lt;p&gt;&lt;a id=&quot;ref4&quot;&gt;&lt;/a&gt;
[4] P. Flajolet, E. Fusy, O. Gandouet, and F. Meunier. Hyperloglog: The analysis of a near-optimal cardinality estimation algorithm. Disc. Math. and Theor. Comp. Sci., AH:127-146, 2007.&lt;/p&gt;
&lt;p&gt;&lt;a id=&quot;ref5&quot;&gt;&lt;/a&gt;
[5] HyperLogLog in Practice: Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm.&lt;/p&gt;
&lt;p&gt;&lt;a id=&quot;ref6&quot;&gt;&lt;/a&gt;
[6] Appendix to HyperLogLog in Practice: Algorithmic Engineering of a State of the Art Cardinality Estimation Algorithm.&lt;/p&gt;
</description> 
        </item> 
        
        <item> 
            <title>准确测量机器学习模型的误差</title> 
            <link>http://blog.codinglabs.org/articles/accurately-measuring-model-prediction-error.html?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=rss</link> 
            <guid>http://blog.codinglabs.org/articles/accurately-measuring-model-prediction-error.html</guid> 
            <author>ericzhang.buaa@gmail.com 张洋</author> 
            <pubDate>Fri, 16 Aug 2013 00:00:00 +0800</pubDate> 
            <description>&lt;p&gt;原文：&lt;a href=&quot;http://scott.fortmann-roe.com/docs/MeasuringError.html&quot;&gt;Accurately Measuring Model Prediction Error&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在机器学习模型的效果评估中，预测误差的分析是重中之重。对于现有的各种误差测量技术，如果使用不当，会得出极具误导性的结论。这些结论会误导模型设计者设计出过拟合的模型，过拟合是指训练出的模型对于训练集拟合的很好，但是对于新的样本集则预测效果极差。这篇文章描述了如何正确的测量模型误差，以避免此类问题。&lt;/p&gt;
&lt;h1 id=&quot;误差测量&quot;&gt;误差测量&lt;/h1&gt;
&lt;p&gt;对于一个预测模型来说，最重要的是要能对&lt;strong&gt;新出现&lt;/strong&gt;的数据样本准确进行预测。所以在测量误差时，必须着重考虑这一点。但是实际中很多模型设计者往往用训练数据的误差而不是新数据的误差来评估模型。这种错误的误差测量方式往往是导致模型质量不高的根源。&lt;/p&gt;
&lt;p&gt;一般来说，模型总是倾向于更好的拟合训练数据。一个模型对于新数据的误差期望总是高于在训练数据上的误差期望。打个比方，例如我们抽取100个人，通过回归模型来预测财富高低对幸福程度的影响。如果我们记下模型对于训练数据进行预测的平方误差（squared error）。然后我们将模型应用于100个新的人进行预测，模型对于新样本的平方误差一般会高于在训练数据上的平方误差。&lt;/p&gt;
&lt;p&gt;下面我们通过公式来更明确的表述这一事实。我们可以建立模型对于新数据的预测误差（我们应该真正关心的指标，也叫实际误差）和模型对于训练数据的预测误差（被很多模型设计者误用的指标）之间的关系。&lt;/p&gt;
&lt;p&gt;\[实际误差=训练集误差+乐观率\]&lt;/p&gt;
&lt;p&gt;这里&lt;em&gt;乐观率&lt;/em&gt;表示相对于训练数据来说，模型在新数据上的表现要糟糕多少。这个指标越高，就表明我们的模型对于训练数据的误差在实际误差中所占的比率越小。&lt;/p&gt;
&lt;h2 id=&quot;过拟合风险&quot;&gt;过拟合风险&lt;/h2&gt;
&lt;p&gt;或许我们可以认为对于一组固定的训练集，乐观率是一个常数。如果这个假设正确，那么我们就可以通过最小化训练集误差来最小化实际误差。也就是说，虽然训练集误差过于乐观，但是如果将其最小化我们仍可以得到一个实际误差最小的模型。因此我们可以忽略实际误差与训练集误差的差异。&lt;/p&gt;
&lt;p&gt;不幸的是上面只是我们一厢情愿的想法。实际情况上乐观率是模型复杂度的函数：随着模型复杂度的增加，乐观率也会随之上升。因此上面的公式可以重新写成：&lt;/p&gt;
&lt;p&gt;\[实际误差=训练集误差+f(模型复杂度)\]&lt;/p&gt;
&lt;p&gt;为什么会这样呢？因为随着模型复杂程度的提高（例如在线性回归中加入越来越多的参数）模型会更倾向于较好的拟合训练数据。这是统计学习模型的基本性质之一&lt;a href=&quot;#ref1&quot;&gt;&lt;sup&gt;1&lt;sup&gt;&lt;/a&gt;。在我们的幸福指数预测模型中，如果我们将人的姓氏也当做一个特征，那么训练数据的误差相比之前就会下降。如果我们将一个已破产公司在1990年1月1日不同时段的股票价格作为特征加入，训练集误差也会下降。甚至我们通过掷骰子随机生成一列数据作为特征加入也可以降低训练集误差。无论增加的特征列与模型有没有关系，训练集误差总会随着特征的增加而下降。&lt;/p&gt;
&lt;p&gt;同时，随着模型复杂度的增加，实际误差也会随之变化（我们实际关心的）。如果我们在幸福指数预测模型中加入了一个公司上世纪某一天的股票价格波动，那么可以预计模型的实际质量将降低。虽然股票价格可以&lt;em&gt;降低训练集误差&lt;/em&gt;，但同时&lt;em&gt;拉高了模型对于新数据的预测误差&lt;/em&gt;，因为加入股票价格特征后模型的稳定性变差了，因此在新数据上的预测表现随之下降。甚至就算你加入了一个与模型相关的预测变量，如果这个变量的信噪比较低，模型的实际误差依然会变大。&lt;/p&gt;
&lt;p&gt;让我们通过一个实际的例子来直观感受一下上面的讨论。假设我们用线性回归模型来做幸福指数预测模型。刚开始我们可以使用最简单的模型：\(Happiness=a+b\ Wealth+\epsilon\)，然后我们逐渐增加多项式项，使得模型可以拟合非线性数据。每增加一项多项式项，模型复杂度也随之提高。通过这个过程，我们可以得到二次模型，如\(Happiness=a+b\ Wealth+c\ Wealth^2+\epsilon\)，或者更复杂的高阶多项式模型：\(Happiness=a+b\ Wealth+c\ Wealth^2+d\ Wealth^3+e\ Wealth^4+f\ Wealth^5+g\ Wealth^6+\epsilon\)。&lt;/p&gt;
&lt;p&gt;下图展示了在不同复杂度下，模型的训练集误差、实际误差及乐观率间的关系。上面的散点图展示了不同复杂度下模型曲线对训练数据的拟合程度。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/accurately-measuring-model-prediction-error/ModelError.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;可见随着模型复杂度上升，训练集误差会随之下降。当模型复杂度很高时，我们的模型可以完美拟合训练集中的所有数据，训练集误差接近0。类似的，实际误差在开始的时候也是在下降。这是因为当模型没有引入高阶多项式项时看起来过于简单，不能很好的拟合训练数据。但是当复杂度到达一个临界点后，随着复杂度继续增加模型对训练数据的拟合越来越好，但是模型对于新数据的预测效果却在变差。&lt;/p&gt;
&lt;p&gt;这种现象叫做&lt;em&gt;过拟合（overfitting）&lt;/em&gt;。在这种情况下，模型过于关注那些训练数据的细节变动，而这些细节变动并不是所有数据共性的东西。从图中可以看出，此时曲线在尽力拟合每一个训练数据，这样的模型显然与训练数据太过紧密的。&lt;/p&gt;
&lt;p&gt;避免过拟合现象是构建精准、鲁棒模型的关键之一。当只关注训练数据时，过拟合现象非常容易发生。为了检测是否存在过拟合，应该将模型应用于新数据上以检测效果。当然了，一般不可能得到真正的实际误差（除非你能得到数据空间的全部数据），但是有诸多方式可以帮助我们对实际误差进行准确估计。本文的第二章节将介绍一些相关的误差估计方法。&lt;/p&gt;
&lt;h2 id=&quot;实例：不合理的误差测量导致的悲剧&quot;&gt;实例：不合理的误差测量导致的悲剧&lt;/h2&gt;
&lt;p&gt;我们通过一个常见的建模流程展示使用训练集误差作为实际误差所带来的陷阱&lt;a href=&quot;#ref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;。我们首先随机生成100个样本数据。每个样本数据有一个目标字段和50个特征字段。例如，目标字段是一种树的生长速率，而特征字段包括降水量、湿度、气压、经纬度等等。在这个例子中，每个样本数据都是完全独立随机生成的，因此这份数据的字段间毫无关系。&lt;/p&gt;
&lt;p&gt;然后我们建立一个线性回归模型来预测生长速率。因为我们知道特征和目标字段没有相关性，所以我们期望得到的结果是\(R^2\)为0。不幸的是我们的模型最后报告\(R^2\)为0.5。这不科学啊！我们的数据明明只是一些噪声数据。不过别急，我们还可以通过&lt;em&gt;F&lt;/em&gt;检验来对模型进行确认。这个可以衡量模型的显著性，用以识别回归出的相关关系是不是只是因为偶然性得到的。&lt;em&gt;F&lt;/em&gt;检验的&lt;em&gt;p&lt;/em&gt;值为0.53，这表明回归模型不显著。&lt;/p&gt;
&lt;p&gt;如果到此为止，一切看起来没有问题；我们应该丢弃这个模型，因为模型并不显著（当然了，这只不过是一些噪声数据！）。不过很多人通常接下来不会彻底丢弃这个模型，而是丢弃那些不显著的特征，然后保留相对显著的特征再次做回归。让我们假设留下显著性水平低于25%的特征，在这个例子中有21个。接着我们再次训练回归模型。&lt;/p&gt;
&lt;p&gt;在第二次训练后，我们得到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;\(R^2\)为0.36&lt;/li&gt;
&lt;li&gt;&lt;em&gt;p&lt;/em&gt;值为\(5*10^{-4}\)&lt;/li&gt;
&lt;li&gt;6个参数的显著性水平达到5%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;再强调一下，我们的数据完全是噪声；不可能有任何相关性。但是我们第二次却得到了一个高显著性的模型，证据就是有意义的\(R^2\)值（在社会科学领域这个值相对较高）和6个显著的参数！&lt;/p&gt;
&lt;p&gt;这是一个令人困惑的结果，我们建模的过程似乎并无不妥，但是却得到了一个匪夷所思的错误结论。这个例子展示了在统计过程中如果不能准确测量误差，则会得到具有严重缺陷的模型。&lt;/p&gt;
&lt;h1 id=&quot;误差的测量方法&quot;&gt;误差的测量方法&lt;/h1&gt;
&lt;h2 id=&quot;使用adjusted-r-2-&quot;&gt;使用Adjusted \(R^2\)&lt;/h2&gt;
&lt;p&gt;\(R^2\)被广泛用于衡量模型的拟合程度。它的计算非常简单。首先对模型进行训练，然后计算每个训练数据实际值与预测值的差，将这些差的平方和相加，然后与&lt;em&gt;零模型&lt;/em&gt;的预测误差平方和做对比。零模型用训练数据集目标字段的的平均值作为预测值。零模型可以看做最简单的预测模型，以此作为基准来评价其它模型的效果。其数学表示如下：&lt;/p&gt;
&lt;p&gt;\[R^2=1-\frac{模型的误差平方和}{零模型的误差平方和}\]&lt;/p&gt;
&lt;p&gt;\(R^2\)的意义非常直观。如果模型的效果并不比零模型号多少，则\(R^2\)接近0，而如果我们的模型效果远好于零模型，则\(R^2\)接近1。\(R^2\)作为一个直观易于理解的评价指标，广泛用于各种回归模型的效果检测。&lt;/p&gt;
&lt;p&gt;通常\(R^2\)是根据模型在训练集上的效果计算的。如前文所示，即使是高\(R^2\)数据本身可能也只是一堆噪声。实际上对于样本容量为n、参数为p的纯噪声数据，\(R^2\)的期望存在一个解析表达式：&lt;/p&gt;
&lt;p&gt;\[E[R^2]=\frac{p}{n}\]&lt;/p&gt;
&lt;p&gt;根据这个公式可以在具体情况下判定\(R^2\)是否有意义。例如上例中，我们的模型有50个参数和100个训练数据，\(R^2\)的期望为\(50/100\)，也就是0.5。&lt;/p&gt;
&lt;p&gt;\(R^2\)有一个变种指标叫做Adjusted \(R^2\)，这个指标会对模型的复杂度做出惩罚。随着参数的增加，Adjusted \(R^2\)对在\(R^2\)的基础上变小。Adjusted \(R^2\)的公式为：&lt;/p&gt;
&lt;p&gt;\[Adjusted R^2=1-(1-R^2)\frac{n-1}{n-p-1}\]&lt;/p&gt;
&lt;p&gt;标准的\(R^2\)会随着模型复杂度增加而变大，而adjusted \(R^2\)克服了这个缺点，因此我们应该总是使用adjusted \(R^2\)而不是\(R^2\)。当然adjusted \(R^2\)也不能完美的评估实际误差。实际上adjusted \(R^2\)一般对模型复杂度的惩罚力度会有所欠缺，所以如果模型足够复杂，adjusted \(R^2\)也会失效。&lt;/p&gt;
&lt;p&gt;因此adjusted \(R^2\)也会出线过拟合现象。另外，adjusted \(R^2\)的许多假设在实际中也不一定成立。这也会导致adjusted \(R^2\)给出错误的结论。&lt;/p&gt;
&lt;p&gt;优点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;便于应用&lt;/li&gt;
&lt;li&gt;内建于许多分析程序中&lt;/li&gt;
&lt;li&gt;计算速度快&lt;/li&gt;
&lt;li&gt;解释性好&lt;a href=&quot;#ref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;缺点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通用性不高&lt;/li&gt;
&lt;li&gt;仍然有过拟合风险&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;信息论方法&quot;&gt;信息论方法&lt;/h2&gt;
&lt;p&gt;有一些方法可以用于评估相对于真实模型来说我们的模型&lt;em&gt;丢失了多少信息&lt;/em&gt;。当然我们是无法获知真实模型的（真正产生训练数据的实际模型），但是在一些前提下我们仍然有办法估计模型的信息丢失程度。信息丢失越多，则模型误差越高，效果越差。&lt;/p&gt;
&lt;p&gt;信息论方法假设模型是一个参数模型（parametric model）。在这个前提下，我们可以根据参数和数据来定义训练数据的似然率（likelihood），不严格的说，似然率是指观测到的这组训练数据出现的概率&lt;a href=&quot;#ref4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;。如果我们调整参数使得这组数据的似然率最大，则得到这组参数的最大似然估计。于是我们就可以利用信息论的方法来比较不同模型和它们的复杂度，以此确定哪个模型最接近真实模型。&lt;/p&gt;
&lt;p&gt;最常用的信息论方法是Akaike信息准则（Akaike&#39;s Information Criteria，简称AIC）。AIC被定义为一个关于模型似然率及模型参数的函数：&lt;/p&gt;
&lt;p&gt;\[AIC=-2ln(Likelihood)+2p\]&lt;/p&gt;
&lt;p&gt;如同其它误差评价准则，我们的目标是最小化AIC。AIC的公式很简洁。第一部分（\(−2ln(Likelihood)\)）可以被视为训练集下的误差率，第二部分（\(2p\)）可视为对模型乐观性的惩罚。&lt;/p&gt;
&lt;p&gt;除了AIC外还有许多基于信息论的判定准则。下面列举两个其它信息准则，与AIC相比其区别在于对乐观性的惩罚方式不同，下面两个准则对乐观性的惩罚还与样本容量n有关。&lt;/p&gt;
&lt;p&gt;\[AICc=−2ln(Likelihood)+2p+\frac{2p(p+1)}{n-p-1}\]&lt;/p&gt;
&lt;p&gt;\[BIC=−2ln(Likelihood)+p ln(n)\]&lt;/p&gt;
&lt;p&gt;如何选择合适的信息准则是非常复杂的，涉及大量理论、实践甚至是哲学因素。实际中决定选用哪个准则要具体情况具体分析，甚至带有一定信仰成分。&lt;/p&gt;
&lt;p&gt;优点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;便于应用&lt;/li&gt;
&lt;li&gt;内建于许多高级分析程序中&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;缺点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要具体情况具体分析&lt;/li&gt;
&lt;li&gt;需要模型能够计算似然率&lt;a href=&quot;#ref5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;学术界对于这种方法的理论基础还存在诸多争议&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;测试集&quot;&gt;测试集&lt;/h2&gt;
&lt;p&gt;上面提到的方法都只能用于参数模型，并且对模型有一些理论假设。如果这些假设不成立，则上面的方法效果将会很查。还好，实践中还有一些其它类型的方法，这些方法对模型没有任何假设，仅仅通过对数据集做处理来估计实际误差。&lt;/p&gt;
&lt;p&gt;其中最简单的方法便是测试集方法。我们首先将样本数据集分为两份。一份用于模型训练；另一份用于效果评测。例如我们有1000个数据，我们可以用700个训练模型，剩下的300个评估模型。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/accurately-measuring-model-prediction-error/holdout.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;这个方法可以说是测量模型误差的标准方法。模型实际误差被定义为模型对于新数据的预测误差。而通过预留测试集，我们可以直接测量这个误差。&lt;/p&gt;
&lt;p&gt;测试集方法的代价是要减少一部分训练数据。例如上面我们从训练集中移除了30%的数据。这意味着相比于使用全量集合训练来说，我们的模型会存在更大的偏差。在标准的流程中，评价完模型效果后，我们会用全量数据重新训练来得到最终的模型。因此在这种情流程下，测试集的误差评价结果是偏&lt;em&gt;保守&lt;/em&gt;的，因为模型的实际误差要比报告的误差低一些。在实际中这种保守的误差估计要比乐观的误差估计更有效。&lt;/p&gt;
&lt;p&gt;这种技术的一个要点是在得到最终模型前不能以任何方式分析或使用测试集。一个常见错误是在效果评估后重新调整模型然后再次训练评估。如果在一次建模中你重复使用一份测试集，这份测试集就被污染了。由于测试集参与了模型调整，它就不能再给出模型误差的一个无偏估计了。&lt;/p&gt;
&lt;p&gt;优点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对模型没有假设&lt;/li&gt;
&lt;li&gt;数据足够多时，准确度较高&lt;/li&gt;
&lt;li&gt;易于实现和使用&lt;/li&gt;
&lt;li&gt;易于理解&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;缺点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;估计偏保守&lt;/li&gt;
&lt;li&gt;一次使用即被污染&lt;/li&gt;
&lt;li&gt;需要确定测试集比例（一般在70%-30%之间）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;交叉验证及重新取样&quot;&gt;交叉验证及重新取样&lt;/h2&gt;
&lt;p&gt;有时，对于模型训练来说保留一部分数据作为测试集的方式有些代价过高。这时一些基于重新取样的方法如交叉验证就比较有用了。&lt;/p&gt;
&lt;p&gt;交叉验证将数据集平均分成n份。例如我们将100个数据分成5份，每份20个数据点。然后我们重复做5轮误差测量。在每轮中，取其中4份（共80个数据点）训练模型，剩下的1份检验模型。然后将5轮测得的误差取平均值，最为对实际误差的估计。&lt;/p&gt;
&lt;p class=&quot;picture&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;/uploads/pictures/accurately-measuring-model-prediction-error/crossvalidation.png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;可以看到，交叉验证与测试集方法很类似。不同之处在于交叉验证中每个数据既参与模型训练又参与模型检测，只不过不在同一轮里。当数据集不是很大时，交叉验证比测试集方法要更值得推荐一些，因为交叉验证不需要移除训练数据。交叉验证同时还能给出误差估计的稳定性度量，这是一个非常有用的指标。不过如果主要目标是衡量估计的稳定性，一些其它的重新取样方法如Bootstrapping更值得一试。&lt;/p&gt;
&lt;p&gt;交叉验证的一个最大问题是确定分组数。一般来说，分组数越小则估计偏差越大（往往偏保守，也就是报告的误差比实际误差要大）但是方差越小。极端情况下，你可以每一个样本点分一个组，这叫做Leave-One-Out-Cross-Validation。此时对误差的估计基本没有偏差，但是方差会很大。理解偏差-方差权衡对于确定分组数是非常重要的。另一个需要关注的点是计算效率。对于每一个分组，你都要训练一个新的模型，所以如果训练过程比较慢的话，还是分少点组为好。最后说一下，根据经验一般把分组数定为5或10是比较合适的选择。&lt;/p&gt;
&lt;p&gt;优点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对模型没有假设&lt;/li&gt;
&lt;li&gt;数据足够多时，准确度较高&lt;/li&gt;
&lt;li&gt;易于理解&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;缺点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算效率低&lt;/li&gt;
&lt;li&gt;需要确定分组数&lt;/li&gt;
&lt;li&gt;估计偏保守&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;做出选择&quot;&gt;做出选择&lt;/h1&gt;
&lt;p&gt;总结一下，我们一共讨论了下列测量模型误差的技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Adjusted \(R^2\)&lt;/li&gt;
&lt;li&gt;信息论方法&lt;/li&gt;
&lt;li&gt;测试集方法&lt;/li&gt;
&lt;li&gt;交叉验证及重新取样&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;作为模型设计者，首先要决定是否依赖前面两个方法对模型的假设条件。如果不是的话，则可以选择后面两个模型。&lt;/p&gt;
&lt;p&gt;一般来说，基于假设条件的模型更便于使用，不过选择这种易用性的同时要付出一些代价。首先就是，对于实际情况来说这些假设都不是完全成立的。至于是否近似成立要具体情况具体分析。很多时候这些假设基本是成立的，不过一旦实际情况与假设出入较大，那么这些方法所得出的结论就完全不可信了。&lt;/p&gt;
&lt;p&gt;就我个人的经验来说，我更偏好交叉验证。因为交叉验证不需要对模型的假设，而且估计效果较好。对于交叉验证来说最主要的消耗是计算资源，不过随着现在计算机计算能力越来越强，这一点可以不用过多担心。对于需要假设的模型来说，虽然实际中很多模型都是参数模型，但是并没有一个有效的方法去判断模型是否符合假设。因此使用这些方法时心里总是存在一点疑虑。而交叉验证虽然计算资源消耗多一点，但是其结论总是更让人放心。&lt;/p&gt;
&lt;h1 id=&quot;footnote&quot;&gt;Footnote&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;仅对于损失函数是凸的（没有局部最大值和最小值）统计模型来说是这样。如果损失函数存在局部最大值或最小值，增加参数会令模型无法收敛到全局最优值，从而导致训练集误差也会变大。不过对于一些常见的模型（如线性回归及逻辑回归）其损失函数都是凸函数。&lt;/li&gt;
&lt;li&gt;这个例子取自Freedman, L. S., &amp; Pee, D. (1989). Return to a note on screening regression equations. The American Statistician, 43(4), 279-282。&lt;/li&gt;
&lt;li&gt;虽然adjusted \(R^2\)与\(R^2\)是不同的统计量，不过两者有类似的直观解释。但是与标准\(R^2\)相比，adjusted \(R^2\)可以是负数（表示这个模型比零模型效果更差）。&lt;/li&gt;
&lt;li&gt;这个定义是不严格的，因为对于连续随机变量，获得这组数据的概率为0。如果让你从0到1之间随机取一个数，则你取到0.724027299329434...的几率为0。你无法准确写出这个数因为其小数部分是无穷的。似然率是通过让模型的概率密度函数取特定值计算出来的。要获得真正的概率，你需要对概率密度函数在一个区间上求积分。因为似然率不是一个概率值，所以它可能大于1。尽管如此，将似然率看做“给定数据集出现的概率”对于直观理解其意义是有帮助的；不过心里要清楚意识到，这在数学上是不准确的！&lt;/li&gt;
&lt;li&gt;这一点限制了信息论方法的适用范围，诸如随机森林与人工神经网络等模型均无法应用此方法。&lt;/li&gt;
&lt;/ol&gt;
</description> 
        </item> 
        
    </channel>
</rss>
